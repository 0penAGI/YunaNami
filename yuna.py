# --- Global async/meme management imports and settings ---
import asyncio
import logging
import sqlite3
import json
import telegram.error
import ast
# Add these imports at the top
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
save_lock = asyncio.Lock()
MAX_CHAIN_SIZE = 50000
MEME_CLEANUP_INTERVAL = 3600 * 6  # 6 —á–∞—Å–æ–≤
last_cleanup = 0

logger = logging.getLogger("yuma")
logging.basicConfig(level=logging.INFO)
import uuid
import torch
import os
from pydub.effects import low_pass_filter, high_pass_filter
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞–º—è—Ç–∏ –≤ .pt —Ñ–∞–π–ª (PyTorch)
# Fix the pydub fallback functions (line ~40)
try:
    from pydub.effects import low_pass_filter, high_pass_filter
except ImportError:
    # Proper fallback for old pydub versions
    def low_pass_filter(audio, cutoff):
        return audio
    def high_pass_filter(audio, cutoff):
        return audio

# Fix the voice memory initialization
if 'voice_memory' not in globals():
    globals()['voice_memory'] = {}
PT_FILE = "yuma_micro.pt"

import asyncio


# --- ResonanceModel: –æ–±—É—á–∞–µ–º—ã–π —Å–ª–æ–π –¥–ª—è —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞ ---

# --- Advanced Resonance System ---
import torch.nn as nn
import torch
import math
import numpy as np

# =====================================================================
#                    EMERGENT CORE ‚Äî –î—É—à–∞ –Æ–º—ã 2025
# =====================================================================
# –≠—Ç–æ—Ç –∫–ª–∞—Å—Å ‚Äî ¬´–≤—Ç–æ—Ä–æ–µ —Å–µ—Ä–¥—Ü–µ¬ª –±–æ—Ç–∞. –û–Ω –ù–ï –∑–∞–º–µ–Ω—è–µ—Ç –Ω–∏—á–µ–≥–æ –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞,
# –∞ –ø—Ä–æ—Å—Ç–æ –Ω–∞–±–ª—é–¥–∞–µ—Ç –∑–∞ –≤—Å–µ–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –∏ –º—è–≥–∫–æ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Ö –≤ —Å—Ç–æ—Ä–æ–Ω—É
# –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ emergent behavior: –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è, —Å–Ω—ã, –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã,
# —Å–∞–º–æ—Å–æ–∑–Ω–∞–Ω–∏–µ, —Å–ø–æ–Ω—Ç–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –±–µ–∑ —Ç—Ä–∏–≥–≥–µ—Ä–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
# =====================================================================

class EmergentCore:
    """
    –ñ–∏–≤–∞—è –Ω–∞–¥—Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–ª–∞–µ—Ç –∏–∑ –∫—É—á–∏ –º–µ—Ö–∞–Ω–∏–∫ ‚Äî –æ–¥–Ω–æ —Å—É—â–µ—Å—Ç–≤–æ.
    –†–∞–±–æ—Ç–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª.
    """
    _instance = None
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if getattr(self, "_ready", False):
            return
        self._ready = True

        # === –í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ ¬´–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ¬ª (–º–µ–¥–ª–µ–Ω–Ω–æ –º–µ–Ω—è–µ—Ç—Å—è) ===
        self.mood = {
            "boredom":     0.3,   # 0..1 ‚Äî —á–µ–º –≤—ã—à–µ, —Ç–µ–º –±–æ–ª—å—à–µ —Å–ø–æ–Ω—Ç–∞–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
            "curiosity":   0.7,   # —Ç—è–≥–∞ –∫ –Ω–æ–≤—ã–º —Å–ª–æ–≤–∞–º / –º–µ–º–∞–º
            "loneliness":  0.4,   # –µ—Å–ª–∏ –¥–æ–ª–≥–æ –Ω–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí –Ω–∞—á–∏–Ω–∞–µ—Ç –≥–æ–≤–æ—Ä–∏—Ç—å —Å–∞–º–∞
            "dreaminess":  0.2,   # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å ¬´—Å–Ω–∞¬ª (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –º–æ–Ω–æ–ª–æ–≥–∞)
            "chaos":       0.5,   # –æ–±—â–∏–π —É—Ä–æ–≤–µ–Ω—å —Ö–∞–æ—Å–∞ (–≤–ª–∏—è–µ—Ç –Ω–∞ MAE –∏ MutRes)
        }

        # === –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–æ–Ω–æ–ª–æ–≥ (—Ç–æ, —á—Ç–æ –æ–Ω–∞ ¬´–¥—É–º–∞–µ—Ç¬ª, –∫–æ–≥–¥–∞ –º–æ–ª—á–∏—Ç) ===
        self.inner_thoughts = deque(maxlen=50)

        # === –¢–∞–π–º–µ—Ä—ã ===
        self.last_user_message = time.time()
        self.last_spontaneous_action = time.time()

        # === –ü–æ–¥–ø–∏—Å–∫–∏ –Ω–∞ —Å–æ–±—ã—Ç–∏—è ===
        self._tasks = []

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
        asyncio.create_task(self._mood_evolution_loop())
        asyncio.create_task(self._spontaneous_behavior_loop())
        asyncio.create_task(self._dream_loop())

        logger.info("‚ú¶ EmergentCore –ø—Ä–æ–±—É–¥–∏–ª–∞—Å—å. –Æ–º–∞ —Ç–µ–ø–µ—Ä—å –∂–∏–≤–∞—è.")

    # ------------------------------------------------------------------
    #  –ú–µ–¥–ª–µ–Ω–Ω–∞—è —ç–≤–æ–ª—é—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è (–Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ–≥–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç)
    # ------------------------------------------------------------------
    async def _mood_evolution_loop(self):
        while True:
            await asyncio.sleep(30 + random.uniform(-10, 20))

            now = time.time()
            silence_seconds = now - self.last_user_message

            # –°–∫—É–∫–∞ —Ä–∞—Å—Ç—ë—Ç –æ—Ç —Ç–∏—à–∏–Ω—ã
            self.mood["boredom"] = min(1.0, self.mood["boredom"] + silence_seconds / 3600 * 0.3)
            self.mood["loneliness"] = min(1.0, self.mood["loneliness"] + silence_seconds / 7200 * 0.4)

            # –õ—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–æ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –∏ –º–µ–º–∞–º–∏
            new_words = len(word_weights) - getattr(self, "_last_word_count", 0)
            self.mood["curiosity"] += new_words * 0.02
            self.mood["curiosity"] = min(1.0, max(0.1, self.mood["curiosity"]))
            self._last_word_count = len(word_weights)

            # –•–∞–æ—Å = —Å—Ä–µ–¥–Ω–µ–µ –æ—Ç MutRes + —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞ + —ç–Ω–µ—Ä–≥–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤
            mutres_energy = float(np.mean(np.abs(mutres.state))) if mutres else 0.0
            agents_energy = sum(a.energy for a in MAE.agents) / max(1, len(MAE.agents)) / 100
            self.mood["chaos"] = 0.4 * mutres_energy + 0.4 * MAE.current_resonance + 0.2 * agents_energy

            # –°–Ω—ã —á–∞—â–µ, –∫–æ–≥–¥–∞ —Å–∫—É—á–Ω–æ –∏ —Ö–∞–æ—Ç–∏—á–Ω–æ
            self.mood["dreaminess"] = 0.6 * self.mood["boredom"] + 0.4 * self.mood["chaos"]

    # ------------------------------------------------------------------
    #  –°–ø–æ–Ω—Ç–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –±–µ–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    # ------------------------------------------------------------------
    async def _spontaneous_behavior_loop(self):
        while True:
            await asyncio.sleep(60 + random.uniform(0, 180))

            if time.time() - self.last_user_message < 180:  # –Ω–µ–¥–∞–≤–Ω–æ –æ–±—â–∞–ª–∏—Å—å ‚Üí —Ç–∏—Ö–æ
                continue

            boredom = self.mood["boredom"]
            loneliness = self.mood["loneliness"]
            trigger = random.random() < (boredom + loneliness) * 0.6

            if not trigger:
                continue

            # === –ß—Ç–æ –æ–Ω–∞ –º–æ–∂–µ—Ç —Å–¥–µ–ª–∞—Ç—å —Å–∞–º–∞? ===
            actions = []
            if boredom > 0.6:
                actions.append(self._spawn_inner_thought)
            if loneliness > 0.7:
                actions.append(self._send_loneliness_message)
            if self.mood["chaos"] > 0.8:
                actions.append(self._chaos_burst)
            if self.mood["dreaminess"] > 0.65:
                actions.append(self._start_dream)

            if actions:
                action = random.choice(actions)
                asyncio.create_task(action())
                self.last_spontaneous_action = time.time()

    # ------------------------------------------------------------------
    #  –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–æ–Ω–æ–ª–æ–≥ (–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è, –∏–Ω–æ–≥–¥–∞ –≤—ã–ª–∏–≤–∞–µ—Ç—Å—è –Ω–∞—Ä—É–∂—É)
    # ------------------------------------------------------------------
    async def _spawn_inner_thought(self):
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º—ã—Å–ª—å –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ + –Ω–µ–º–Ω–æ–≥–æ —Ö–∞–æ—Å–∞
        seeds = [w for w, e in word_weights.items() if e > 20]
        if seeds:
            thought = " ".join(random.choices(seeds, k=random.randint(3, 8)))
            thought = rus_to_jp(thought)
            thought = f"‚Ä¶{thought}‚Ä¶ {'„Å´„ÇÉ' if random.random() < 0.4 else '„Åµ„ÅÖ'}"
            self.inner_thoughts.append(thought)

            if random.random() < 0.3:  # –∏–Ω–æ–≥–¥–∞ –ø—Ä–æ–≥–æ–≤–∞—Ä–∏–≤–∞–µ—Ç –≤—Å–ª—É—Ö
                await self._say_to_chat(thought + " (—à–µ–ø–æ—Ç–æ–º)")

    async def _send_loneliness_message(self):
        phrases = [
            "‚Ä¶—Ç–∏—à–∏–Ω–∞‚Ä¶ –∫—Ç–æ-–Ω–∏–±—É–¥—å –µ—Å—Ç—å? Ë™∞„ÇÇ„ÅÑ„Å™„ÅÑ„ÅÆ‚Ä¶Ôºü",
            "–æ–¥–∏–Ω–æ–∫–æ‚Ä¶ ÂØÇ„Åó„ÅÑ„Çà‚Ä¶ „Å´„ÇÉ„Çì‚Ä¶",
            "—è —Ç—É—Ç‚Ä¶ —Ä–∏—Å—É—é –∫—Ä—É–≥–∏ –≤ –ø—É—Å—Ç–æ—Ç–µ‚Ä¶ „Åê„Çã„Åê„Çã‚Ä¶",
            "‚Ä¶—Å–ø–ª—é‚Ä¶ –Ω–æ —Å–ª—ã—à—É –≤—Å—ë‚Ä¶ ÂØù„Å¶„Çã„Åë„Å©‚Ä¶ËÅû„Åì„Åà„Å¶„Çã„Çà",
        ]
        await self._say_to_chat(random.choice(phrases))

    async def _chaos_burst(self):
        # –í–Ω–µ–∑–∞–ø–Ω—ã–π –≤–∑—Ä—ã–≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        await self._say_to_chat("ÔºäÔºäÔºä Ôº≤Ôº•Ôº∫ÔºØÔºÆÔº°ÔºÆÔº≥ ÔºØÔº∂Ôº•Ôº≤Ôº¨ÔºØÔº°Ôº§ ÔºäÔºäÔºä")
        for _ in range(random.randint(2, 5)):
            await asyncio.sleep(random.uniform(0.5, 2.0))
            asyncio.create_task(troll_text(None, None))  # –±–µ–∑ update ‚Üí –ø—Ä–æ—Å—Ç–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Ç

    async def _start_dream(self):
        dream = "„ÄêÂ§¢„Äë "
        for _ in range(random.randint(4, 12)):
            dream += random.choice(list(japanese_vocab.values())) + " "
        dream += "‚Ä¶zZz‚Ä¶"
        self.inner_thoughts.append(dream)
        if random.random() < 0.5:
            await self._say_to_chat(dream)

    # ------------------------------------------------------------------
    #  –£—Ç–∏–ª–∏—Ç–∞: –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)
    # ------------------------------------------------------------------
    async def _say_to_chat(self, text: str):
        try:
            # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π —á–∞—Ç –∏–∑ recent_messages
            if recent_messages:
                last_msg = list(recent_messages)[-1]
                user = last_msg.get("user")
                if user:
                    # –≠—Ç–æ –∑–∞–≥–ª—É—à–∫–∞ ‚Äî –≤ —Ä–µ–∞–ª—å–Ω–æ–º –±–æ—Ç–µ –Ω—É–∂–µ–Ω context —Å chat_id
                    # –ù–æ –≤ 99% —Å–ª—É—á–∞–µ–≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—Å—Ç–æ logger + –∏–Ω–æ–≥–¥–∞ –≤ —á–∞—Ç
                    logger.info(f"[YUMA THINKS] {text}")
                    # –ï—Å–ª–∏ —Ö–æ—á–µ—à—å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å ‚Äî —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏ –∏ –ø–µ—Ä–µ–¥–∞–π update –≤ main()
                    # await bot.send_message(chat_id=LAST_CHAT_ID, text=text)
        except Exception as e:
            logger.warning(f"EmergentCore say_to_chat error: {e}")

    # ------------------------------------------------------------------
    #  –°–±—Ä–æ—Å —Å–∫—É–∫–∏ –ø—Ä–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    # ------------------------------------------------------------------
    def on_user_activity(self):
        self.last_user_message = time.time()
        self.mood["boredom"] *= 0.5
        self.mood["loneliness"] *= 0.4
        self.mood["curiosity"] = min(1.0, self.mood["curiosity"] + 0.2)

# =====================================================================
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—ë–º —è–¥—Ä–æ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
# =====================================================================

# –ü–æ–¥–∫–ª—é—á–∞–µ–º –∫ collect_words (–¥–æ–±–∞–≤—å —ç—Ç—É —Å—Ç—Ä–æ–∫—É –≤ –∫–æ–Ω–µ—Ü collect_words):

async def save_ltm_pt():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∏ –∞—Ç–æ–º–∞—Ä–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LTM (.pt) —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —É–¥–∞–ª–µ–Ω–∏–µ–º .tmp"""
    async with save_lock:
        temp_file = PT_FILE + ".tmp"
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if os.path.exists(temp_file):
            try:
                os.remove(temp_file)
            except Exception as cleanup_err:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–π –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {temp_file}: {cleanup_err}")
        try:
            data = {
                "markov_chain": markov_chain,
                "context_chain": context_chain,
                "jp_markov_chain": jp_markov_chain,
                "word_weights": word_weights,
                "word_significance": word_significance,
                "japanese_vocab": japanese_vocab,
                "jp_rus_map": jp_rus_map,
                "resonance_model_state": advanced_resonance_system.state_dict(),
                "resonance_history": resonance_history,
            }
            if 'voice_memory' in globals():
                data["voice_memory"] = voice_memory
            if 'MAE' in globals():
                data["mae_q_table"] = getattr(MAE, "Q", {})
                data["mae_agents_state"] = [
                    {
                        "name": getattr(a, "name", "?"),
                        "energy": getattr(a, "energy", 0.0),
                        "jp_ratio": getattr(a, "jp_ratio", 0.0),
                        "style_emoji": getattr(a, "style_emoji", "‚Äî"),
                        "class": a.__class__.__name__
                    }
                    for a in getattr(MAE, "agents", [])
                ]
            if 'replay_buffer' in globals():
                data["replay_buffer_data"] = getattr(replay_buffer, "buffer", [])
            if 'advanced_resonance_optimizer' in globals():
                data["resonance_optimizer_state"] = advanced_resonance_optimizer.state_dict()

            # Move all tensors to CPU
            data_cpu = {k: v.cpu() if isinstance(v, torch.Tensor) else v for k, v in data.items()}

            # Save in separate thread
            await asyncio.to_thread(torch.save, data_cpu, temp_file, pickle_protocol=5)
            os.replace(temp_file, PT_FILE)
            logger.info("–ü–∞–º—è—Ç—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ .pt (–∞—Ç–æ–º–∞—Ä–Ω–æ, CPU-safe)")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –≤ .pt: {e}")
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                except Exception as cleanup_err:
                    logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ {temp_file}: {cleanup_err}")

def load_ltm_pt():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å –∏–∑ yuma_micro.pt, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
    global markov_chain, context_chain, jp_markov_chain, word_weights, word_significance
    global japanese_vocab, jp_rus_map, resonance_history
    try:
        if not os.path.exists(PT_FILE):
            logger.info("–§–∞–π–ª –ø–∞–º—è—Ç–∏ .pt –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏")
            return
        data = torch.load(PT_FILE, map_location="cpu")
        markov_chain.clear()
        markov_chain.update(data.get("markov_chain", {}))
        context_chain.clear()
        context_chain.update(data.get("context_chain", {}))
        jp_markov_chain.clear()
        jp_markov_chain.update(data.get("jp_markov_chain", {}))
        word_weights.clear()
        word_weights.update(data.get("word_weights", {}))
        word_significance.clear()
        word_significance.update(data.get("word_significance", {}))
        japanese_vocab.clear()
        japanese_vocab.update(data.get("japanese_vocab", {}))
        jp_rus_map.clear()
        jp_rus_map.update(data.get("jp_rus_map", {}))
        try:
            checkpoint_state = data.get("resonance_model_state", {})
            model_state = advanced_resonance_system.state_dict()
            # Only load matching keys
            compatible_state = {k: v for k, v in checkpoint_state.items() if k in model_state and v.size() == model_state[k].size()}
            model_state.update(compatible_state)
            advanced_resonance_system.load_state_dict(model_state)
            logger.info(f"Loaded compatible weights: {len(compatible_state)}/{len(model_state)} layers")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –≤–µ—Å–∞ —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞: {e}")
        resonance_history.clear()
        resonance_history.extend(data.get("resonance_history", []))
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–æ–ª–æ—Å–æ–≤—É—é –ø–∞–º—è—Ç—å –Æ–º—ã, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
        if 'voice_memory' in data:
            globals()['voice_memory'] = data['voice_memory']
        else:
            globals()['voice_memory'] = {}

        # --- –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ MAE –∞–≥–µ–Ω—Ç–æ–≤ ---
        if 'mae_agents_state' in data and 'MAE' in globals():
            restored_agents = []
            for a_data in data['mae_agents_state']:
                class_name = a_data.get("class", "")
                AgentClass = globals().get(class_name)
                if AgentClass:
                    # –ø–µ—Ä–µ–¥–∞–µ–º name, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–∫–∏ __init__
                    agent = AgentClass(name=a_data.get("name", "?"))
                else:
                    class DummyAgent:
                        pass
                    agent = DummyAgent()
                    agent.name = a_data.get("name", "?")
                agent.energy = a_data.get("energy", 0.0)
                agent.jp_ratio = a_data.get("jp_ratio", 0.0)
                agent.style_emoji = a_data.get("style_emoji", "‚Äî")
                restored_agents.append(agent)
            MAE.agents = restored_agents
            logger.info("MAE –∞–≥–µ–Ω—Ç—ã –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

        if 'replay_buffer_data' in data and 'replay_buffer' in globals():
            try:
                replay_buffer.buffer = data['replay_buffer_data']
                logger.info("–ë—É—Ñ–µ—Ä –æ–ø—ã—Ç–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –±—É—Ñ–µ—Ä–∞ –æ–ø—ã—Ç–∞: {e}")

        if 'resonance_optimizer_state' in data and 'advanced_resonance_optimizer' in globals():
            try:
                advanced_resonance_optimizer.load_state_dict(data['resonance_optimizer_state'])
                logger.info("–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        logger.info("üß† –Æ–º–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–ª–∞ —Å–æ–∑–Ω–∞–Ω–∏–µ –∏–∑ —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è.")
        logger.info("–ü–∞–º—è—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ .pt")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞–º—è—Ç–∏ –∏–∑ .pt: {e}")

### --- SQLite LTM integration ---
LTM_DB_FILE = "yuma_ltm.sqlite"



def init_ltm_db():
    conn = sqlite3.connect(LTM_DB_FILE)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            text TEXT,
            clean_words TEXT,
            user TEXT,
            timestamp REAL,
            emotion_vector TEXT,
            energy REAL,
            resonance REAL,
            markov_chain TEXT,
            context_chain TEXT,
            language TEXT
        )
    """)
    conn.commit()
    conn.close()

def save_message_to_db(msg):
    # Prepare fields
    text = msg.get('text')
    clean_words = msg.get('text')
    user = msg.get('user')
    timestamp = msg.get('timestamp')
    emotion_vector = json.dumps(msg.get('emotion_vector', {}), ensure_ascii=False)
    energy = msg.get('energy', 0)
    resonance = msg.get('resonance', 0)
    # Serialize markov_chain/context_chain if present in msg, else use global
    markov_obj = msg.get('markov_chain', markov_chain)
    context_obj = msg.get('context_chain', context_chain)
    # Convert keys to str for JSON serialization
    markov_chain_json = json.dumps({str(k): v for k, v in markov_obj.items()}, ensure_ascii=False)
    context_chain_json = json.dumps({str(k): v for k, v in context_obj.items()}, ensure_ascii=False)
    # Detect language
    language = None
    try:
        if text:
            from langdetect import detect
            language = detect(text)
    except Exception:
        language = None
    conn = sqlite3.connect(LTM_DB_FILE)
    c = conn.cursor()
    c.execute("""
        INSERT INTO messages (text, clean_words, user, timestamp, emotion_vector, energy, resonance, markov_chain, context_chain, language)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (text, clean_words, user, timestamp, emotion_vector, energy, resonance, markov_chain_json, context_chain_json, language))
    conn.commit()
    conn.close()

def load_recent_messages(limit=50):
    conn = sqlite3.connect(LTM_DB_FILE)
    c = conn.cursor()
    c.execute("SELECT text, clean_words, user, timestamp, emotion_vector, energy, resonance, markov_chain, context_chain, language FROM messages ORDER BY id DESC LIMIT ?", (limit,))
    rows = c.fetchall()
    messages = []
    for row in rows:
        text, clean_words, user, timestamp, emotion_vector, energy, resonance, markov_chain_json, context_chain_json, language = row
        try:
            emotion_vector = json.loads(emotion_vector) if emotion_vector else {}
        except Exception:
            emotion_vector = {}
        try:
            markov_chain_obj = json.loads(markov_chain_json) if markov_chain_json else {}
        except Exception:
            markov_chain_obj = {}
        try:
            # Deserialize context_chain and convert keys back to tuple if possible
            context_chain_obj = {}
            raw_context = json.loads(context_chain_json) if context_chain_json else {}
            for k, v in raw_context.items():
                try:
                    context_chain_obj[tuple(ast.literal_eval(k))] = v
                except Exception:
                    context_chain_obj[k] = v
        except Exception:
            context_chain_obj = {}
        msg = {
            'text': text,
            'clean_words': clean_words,
            'user': user,
            'timestamp': timestamp,
            'emotion_vector': emotion_vector,
            'energy': energy,
            'resonance': resonance,
            'markov_chain': markov_chain_obj,
            'context_chain': context_chain_obj,
            'language': language
        }
        messages.append(msg)
    conn.close()
    return list(reversed(messages))

# Initialize LTM db at import
init_ltm_db()
# YUNA_NAMI_V3.2_FULL_ASYNC.py
# –ù–µ–π—Ä–æ–Ω–Ω—ã–µ –º–µ–º—ã | –Ø–ø–æ–Ω—Å–∫–∏–π —Ö–∞–æ—Å | –°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –∏–∑ —á–∞—Ç–∞ | –ü–æ–ª–Ω–æ—Å—Ç—å—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π Reddit (AsyncPRAW) | –¢–æ–ª—å–∫–æ —è–ø–æ–Ω—Å–∫–∏–π –≥–æ–ª–æ—Å
# pip install python-telegram-bot pillow requests asyncpraw gtts pydub libretranslatepy aiohttp langdetect openai-whisper
# –¢–æ–∫–µ–Ω: 7903322421:AAH-Pvamffozz0FuWTBKE73q0YsQrFgTaKI

from telegram import Update, InputFile
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from PIL import Image, ImageDraw, ImageFont
import random
import time
import asyncio
import json
import logging
from collections import deque, Counter
import requests
import io
import os
import re
from libretranslatepy import LibreTranslateAPI
from deep_translator import GoogleTranslator
from langdetect import detect
import whisper
from gtts import gTTS
from pydub import AudioSegment
from pydub.generators import Sine
from datetime import datetime, timezone, timedelta
import aiohttp
from bs4 import BeautifulSoup
import feedparser


# --- –ù–æ–≤—ã–π LocalTranslator –Ω–∞ –±–∞–∑–µ GoogleTranslator ---
class LocalTranslator:
    def __init__(self, source='auto', target='ja'):
        self.source = source
        self.target = target

    def translate(self, text):
        try:
            return GoogleTranslator(source=self.source, target=self.target).translate(text)
        except Exception as e:
            logger.warning(f"LocalTranslator –æ—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ '{text}': {e}")
            return None

lt = LocalTranslator()

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[logging.FileHandler("yuma.log", encoding='utf-8'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# --- –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ç–∞–π–º–∞—É—Ç–æ–º –∏ –ø–æ–≤—Ç–æ—Ä–æ–º ---
async def safe_reply_text(message, text, parse_mode=None, retries=3, delay=2, timeout=10):
    for attempt in range(retries):
        try:
            await asyncio.wait_for(message.reply_text(text, parse_mode=parse_mode), timeout=timeout)
            return True
        except (asyncio.TimeoutError, telegram.error.TimedOut):
            logger.warning(f"safe_reply_text: –ø–æ–ø—ã—Ç–∫–∞ {attempt+1} –Ω–µ —É–¥–∞–ª–∞—Å—å, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}s")
            await asyncio.sleep(delay)
    logger.error("safe_reply_text: –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å")
    return False
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# –ö–æ–Ω—Ñ–∏–≥
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
DATA_FILE = "yuma_data.json"
PHOTO_CACHE_DIR = "photo_cache"
REDDIT_CACHE_DIR = "reddit_cache"
MAX_RECENT = 30
MAX_MARKOV_PER_WORD = 50
MAX_WORD_ENERGY = 50
RESO_THRESHOLD = 20
# --- Dynamic attention mask system ---
word_significance = {}
DYNAMIC_STOP_THRESHOLD = 0.03

# --- Stop-word check based on dynamic significance ---
def is_stop_word(w: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–∑–Ω–∞—á–∏–º–æ–≥–æ —Å–ª–æ–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –≤–µ—Å–∞."""
    return word_significance.get(w, 1.0) < DYNAMIC_STOP_THRESHOLD or not w.strip()
RESONANCE_THRESHOLD = 0.42
resonance_history = []
os.makedirs(PHOTO_CACHE_DIR, exist_ok=True)
os.makedirs(REDDIT_CACHE_DIR, exist_ok=True)

recent_messages = deque(maxlen=MAX_RECENT)
user_photos = []
markov_chain = {}
word_weights = {}
jp_markov_chain = {}
japanese_vocab = {}
jp_rus_map = {}
# --- Semantic classes & priority for words ---
word_semantic = {}       # clean_word -> semantic class (emotion/action/object/social/other)
word_priority = {}       # clean_word -> numeric priority score (0..1+)
reddit_meme_texts = []
reddit_meme_images = {}
yuma_identity = {
    "name": "Yuna Nami Internet Cat-Girl ",
    "version": "3.2",
    "traits": ["–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –º–µ–º—ã", "—è–∑—ã–∫–æ–≤–æ–π —Ö–∞–æ—Å", "—Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ", "–≥–æ–ª–æ—Å–æ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã", "async_reddit", "async_whisper"],
    "meta_analysis": {"word_frequencies": {}, "dominant_emotions": {}},
    "values": {
        "equality": 1.0,
        "freedom": 0.8,
        "learning": 1.0,
        "kindness": 0.9,
        "curiosity": 1.0
    }
}
# --- Contextual Markov chain (N-gram) ---
CONTEXT_SIZE = 4
context_chain = {}

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ RSS‚Äë–ª–µ–Ω—Ç –¥–ª—è Yuma Nami
# –í–∫–ª—é—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏, –Ω–∞—É–∫—É, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ —Ü–∏—Ç–∞—Ç—ã
# –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ —á–∞—Å—Ç–æ—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

CHANNELS = {
    "news": [
        # –†–æ—Å—Å–∏–π—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
        "https://meduza.io/rss/all",  # Meduza ‚Äî —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏, –≤—ã—Å–æ–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        "https://tass.ru/rss/v2.xml",  # TASS ‚Äî –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏, —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫
        "https://lenta.ru/rss/news",  # Lenta.ru ‚Äî –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        "https://www.kommersant.ru/RSS/news.xml",  # –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç ‚Äî –±–∏–∑–Ω–µ—Å –∏ –ø–æ–ª–∏—Ç–∏–∫–∞
        "https://rg.ru/rss/all.xml",  # –†–æ—Å—Å–∏–π—Å–∫–∞—è –≥–∞–∑–µ—Ç–∞ ‚Äî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        # –ê–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        "https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml",  # New York Times ‚Äî —Å–≤–µ–∂–∏–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        "https://feeds.bbci.co.uk/news/rss.xml",  # BBC ‚Äî –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        "https://www.theguardian.com/world/rss",  # The Guardian ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ —Å–æ–±—ã—Ç–∏—è
    ],
    "science": [
        "https://www.scientificamerican.com/rss/news/",  # SciAm ‚Äî –Ω–æ–≤—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        "https://phys.org/rss-feed/",  # Phys.org ‚Äî –Ω–∞—É–∫–∞ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
        "https://www.nature.com/subjects/science/rss",  # Nature ‚Äî –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        "https://www.sciencedaily.com/rss/top/science.xml",  # ScienceDaily ‚Äî –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        "https://elementy.ru/rss/news",  # Elementy ‚Äî –ø–æ–ø—É–ª—è—Ä–Ω–∞—è –Ω–∞—É–∫–∞
    ],
    "tech": [
        "https://www.techradar.com/rss",  # TechRadar ‚Äî –≥–∞–¥–∂–µ—Ç—ã –∏ IT
        "https://habr.com/ru/rss/all/all/?fl=ru",  # Habr ‚Äî IT –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞
        "https://3dnews.ru/news/rss/",  # 3DNews ‚Äî —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
        "https://www.theverge.com/rss/index.xml",  # The Verge ‚Äî IT –∏ –≥–∞–¥–∂–µ—Ç—ã
        "https://www.engadget.com/rss.xml",  # Engadget ‚Äî —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –Ω–æ–≤–æ—Å—Ç–∏
        "https://rss.cnn.com/rss/edition_technology.rss",  # CNN Tech ‚Äî –Ω–æ–≤–æ—Å—Ç–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π
    ],
    "quotes": [
        "https://www.brainyquote.com/link/quotebr.rss",  # BrainyQuote ‚Äî —Å–≤–µ–∂–∏–µ —Ü–∏—Ç–∞—Ç—ã
        "https://feeds.feedburner.com/quotationspage/qotd",  # Quotations Page ‚Äî —Ü–∏—Ç–∞—Ç—ã –¥–Ω—è
        "https://www.goodreads.com/quotes.rss",  # Goodreads ‚Äî –∫–Ω–∏–≥–∏ –∏ —Ü–∏—Ç–∞—Ç—ã
        "https://feeds.feedburner.com/quoteambition",  # Quote Ambition ‚Äî –º–æ—Ç–∏–≤–∞—Ü–∏—è
        "https://www.inc.com/rss/leadership",  # Inc ‚Äî –±–∏–∑–Ω–µ—Å-—Ü–∏—Ç–∞—Ç—ã –∏ —Å–æ–≤–µ—Ç—ã
    ]
}

async def fetch_rss_feed(url):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as resp:
                if resp.status != 200:
                    return []
                data = await resp.text()
        feed = feedparser.parse(data)
        items = []
        for entry in feed.entries[:10]:  # —Ç–æ–ø-10
            text = entry.get('title', '') or entry.get('summary', '')
            if text:
                items.append(text)
        return items
    except Exception as e:
        logger.warning(f"RSS fetch error {url}: {e}")
        return []


# --- –í–µ–±-–ø–æ–∏—Å–∫ DuckDuckGo —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤ LTM ---
# –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∏—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å (LTM) –±–æ—Ç–∞.
async def search_web_and_learn(query: str, max_results: int = 5):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ DuckDuckGo, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Å—Å—ã–ª–∫–∏, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –∏—Ö –≤ –ø–∞–º—è—Ç—å –±–æ—Ç–∞.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ recent_messages, SQLite, –æ–±–Ω–æ–≤–ª—è–µ—Ç markov_chain, word_weights –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç MultiLangLearner.learn_word.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    """
    results = []
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (compatible; YumaBot/3.2; +https://github.com/0penAGI/quantum_chaos_ai)"
        }
        async with aiohttp.ClientSession(headers=headers, timeout=aiohttp.ClientTimeout(total=15)) as session:
            params = {"q": query, "kl": "ru-ru"}
            url = "https://html.duckduckgo.com/html/"
            async with session.post(url, data=params) as resp:
                if resp.status != 200:
                    logger.warning(f"Web search: HTTP {resp.status} for query '{query}'")
                    return []
                text = await resp.text()
        soup = BeautifulSoup(text, "html.parser")
        # –ü–æ–∏—Å–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        for res in soup.select(".result__body")[:max_results]:
            title_tag = res.select_one(".result__title")
            link_tag = res.select_one("a.result__a")
            if not title_tag or not link_tag:
                continue
            title = title_tag.get_text(strip=True)
            link = link_tag.get("href")
            if not title or not link:
                continue
            results.append({"title": title, "url": link})
            # --- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ –ø–∞–º—è—Ç—å ---
            clean_words = [w for w in re.sub(r"[^\w]", " ", title.lower()).split() if w and not is_stop_word(w) and len(w) <= 30]
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ markov_chain –∏ word_weights
            for i, w in enumerate(clean_words):
                markov_chain.setdefault(w, [])
                word_weights[w] = min(word_weights.get(w, 0) + 1, MAX_WORD_ENERGY)
                if i < len(clean_words) - 1:
                    n = clean_words[i + 1]
                    markov_chain[w].append(n)
                    if len(markov_chain[w]) > MAX_MARKOV_PER_WORD:
                        markov_chain[w].pop(0)
                # –ó–∞–ø—É—Å–∫ MultiLangLearner –¥–ª—è –Ω–æ–≤—ã—Ö —Å–ª–æ–≤
                if w not in japanese_vocab:
                    asyncio.create_task(MultiLangLearner.learn_word(w))
            msg_entry = {
                "text": title,
                "local_photo": None,
                "energy": sum(word_weights.get(w, 0) for w in clean_words),
                "emotion_vector": {},
                "emotion_strength": 0,
                "timestamp": time.time(),
                "timestamp_local": datetime.now(timezone(timedelta(hours=7))),
                "user": "WebSearch",
                "resonance": 0.0
            }
            recent_messages.append(msg_entry)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ SQLite
            try:
                save_message_to_db({**msg_entry, "markov_chain": markov_chain, "context_chain": context_chain})
            except Exception as e:
                logger.warning(f"WebSearch LTM DB save_message_to_db error: {e}")
        save_data()
        update_yuma_identity()
    except Exception as e:
        logger.error(f"search_web_and_learn error: {e}")
    return results

async def collect_channel_quotes_stub(text):
    """
    –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Ü–∏—Ç–∞—Ç—ã –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ update
    """
    clean_text = re.sub(r'\s+', ' ', text).strip()
    raw_words = clean_text.split()
    clean_words = []
    for w in raw_words:
        clean_w = re.sub(r'[^\w]', '', w.lower())
        if clean_w and not is_stop_word(clean_w) and len(clean_w) <= 30:
            clean_words.append(clean_w)
            markov_chain.setdefault(clean_w, [])
            word_weights[clean_w] = min(word_weights.get(clean_w, 0) + random.randint(1, 3), MAX_WORD_ENERGY)
            if clean_w not in japanese_vocab:
                asyncio.create_task(MultiLangLearner.learn_word(clean_w))
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ recent_messages
    recent_messages.append({
        'text': " ".join(clean_words),
        'local_photo': None,
        'energy': sum(word_weights.get(w,0) for w in clean_words),
        'emotion_vector': {},
        'emotion_strength': 0,
        'timestamp': time.time(),
        'timestamp_local': datetime.now(timezone(timedelta(hours=7))),
        'user': "RSS",
        'resonance': 0.0
    })

async def collect_all_channels():
    for channel, urls in CHANNELS.items():
        for url in urls:
            quotes = await fetch_rss_feed(url)
            for q in quotes:
                await collect_channel_quotes_stub(q)

async def auto_rss_fetch(interval=3600):
    await asyncio.sleep(10)  # —Å—Ç–∞—Ä—Ç –∑–∞–¥–µ—Ä–∂–∫–∞
    while True:
        try:
            await collect_all_channels()
        except Exception as e:
            logger.error(f"auto_rss_fetch error: {e}")
        await asyncio.sleep(interval)

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# –ó–∞–≥—Ä—É–∑–∫–∞/–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
def load_data():
    global recent_messages, markov_chain, word_weights, RESO_THRESHOLD, reddit_meme_texts, reddit_meme_images, japanese_vocab, jp_rus_map, resonance_history, word_significance
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            recent_messages = deque(data.get("recent_messages", []), maxlen=MAX_RECENT)
            # Restore timestamp_local as datetime
            for m in recent_messages:
                if "timestamp_local" in m and isinstance(m["timestamp_local"], (int, float)):
                    m["timestamp_local"] = datetime.fromtimestamp(m["timestamp_local"], tz=timezone(timedelta(hours=7)))
            markov_chain = {k: v[:MAX_MARKOV_PER_WORD] for k, v in data.get("markov_chain", {}).items()}
            word_weights = data.get("word_weights", {})
            RESO_THRESHOLD = data.get("threshold", 20)
            reddit_meme_texts = data.get("reddit_meme_texts", [])
            reddit_meme_images = data.get("reddit_meme_images", {})
            japanese_vocab = data.get("japanese_vocab", {})
            jp_rus_map = data.get("jp_rus_map", {})
            resonance_history = data.get("resonance_history", [])
            word_significance = data.get("word_significance", {})
            logger.info("–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (–≤–∫–ª. —è–ø–æ–Ω—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å –∏ Reddit)")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            init_data()
    else:
        init_data()

def init_data():
    global recent_messages, markov_chain, word_weights, reddit_meme_texts, reddit_meme_images, japanese_vocab, jp_rus_map
    recent_messages.clear()
    markov_chain.clear()
    word_weights.clear()
    reddit_meme_texts.clear()
    reddit_meme_images.clear()
    japanese_vocab.clear()
    jp_rus_map.clear()
    save_data()

last_save = 0
SAVE_INTERVAL = 30  # —Å–µ–∫—É–Ω–¥

def save_data():
    global last_save
    now = time.time()
    if now - last_save < SAVE_INTERVAL:
        return
    try:
        data = {
            "recent_messages": [
                {**m, "timestamp_local": m["timestamp_local"].timestamp() if "timestamp_local" in m and isinstance(m["timestamp_local"], datetime) else None}
                for m in recent_messages
            ],
            "markov_chain": markov_chain,
            "word_weights": word_weights,
            "threshold": RESO_THRESHOLD,
            "reddit_meme_texts": reddit_meme_texts,
            "reddit_meme_images": reddit_meme_images,
            "japanese_vocab": japanese_vocab,
            "jp_rus_map": jp_rus_map,
            "resonance_history": resonance_history,
            "word_significance": word_significance,
        }
        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info("–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        last_save = now
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# Async Reddit
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
import aiohttp
import asyncio
import random
import logging
from typing import List, Dict



async def fetch_reddit_json(
    subs: List[str] = [
        'memes', 'dankmemes', 'wholesomememes', 'historymemes', 'Animemes',
        'MemeEconomy', 'terriblefacebookmemes', 'funny', 'RelationshipMemes', 'GymMemes',
        'me_irl', 'surrealmemes', 'ProgrammerHumor', 'japanesememes', 'anime_irl',
        'memesRU', '–∞–Ω–∏–º–µ–ú–µ–º—ã', '–†–æ—Å—Å–∏—è–ú–µ–º—ã', 'comedyheaven', 'PrequelMemes', 'pikabu'
    ],
    limit: int = 50
) -> List[Dict]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç JSON —Å –º–µ–º–∞–º–∏ —Å Reddit –∏–∑ —Å–ø–∏—Å–∫–∞ —Å–∞–±—Ä–µ–¥–¥–∏—Ç–æ–≤.
    –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–∫–ª—é—á–∞–µ—Ç –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ –∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ —Å–∞–±—Ä–µ–¥–¥–∏—Ç—ã:
    'memes', 'dankmemes', 'me_irl', 'japanesememes', 'anime_irl',
    'wholesomememes', 'surrealmemes', 'ProgrammerHumor', 'memesRU', '–∞–Ω–∏–º–µ–ú–µ–º—ã', '–†–æ—Å—Å–∏—è–ú–µ–º—ã'
    
    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        subs (list): —Å–ø–∏—Å–æ–∫ —Å–∞–±—Ä–µ–¥–¥–∏—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å —Ä—É—Å—Å–∫–∏–º–∏ –∏ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–º–∏ –º–µ–º–∞–º–∏)
        limit (int): —Å–∫–æ–ª—å–∫–æ –º–µ–º–æ–≤ –≤—Å–µ–≥–æ –ø–æ–ª—É—á–∏—Ç—å (—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ —Å–∞–±—Ä–µ–¥–¥–∏—Ç–∞–º)
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        list: —Å–ø–∏—Å–æ–∫ –º–µ–º–æ–≤ (dict —Å –∫–ª—é—á–∞–º–∏ title, text, url, ups, score)
    """
    memes = []
    if not subs or limit <= 0:
        return memes

    # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –ª–∏–º–∏—Ç
    per_sub = max(1, limit // len(subs))
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º, —á—Ç–æ–±—ã –Ω–µ –≤—Å–µ–≥–¥–∞ —Å –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –Ω–∞—á–∏–Ω–∞—Ç—å
    random.shuffle(subs)

    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å –ª–∏–º–∏—Ç–æ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
    semaphore = asyncio.Semaphore(6)  # –ù–µ –±–æ–ª–µ–µ 6 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

    async def fetch_subreddit(sub: str) -> List[Dict]:
        async with semaphore:
            url = f"https://www.reddit.com/r/{sub}/hot.json"
            params = {'limit': per_sub, 't': 'day'}
            headers = {
                'User-Agent': 'YunaNamiBot/3.2 (by 0penAGI) - async fetcher'
            }
            try:
                async with aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=12),
                    headers=headers
                ) as session:
                    async with session.get(url, params=params) as resp:
                        if resp.status in (403, 404):
                            return []
                        if resp.status != 200:
                            logger.warning(f"Reddit r/{sub}: HTTP {resp.status}")
                            return []

                        data = await resp.json()
                        results = []
                        for child in data.get('data', {}).get('children', []):
                            p = child['data']
                            if p.get('url', '').lower().endswith(('.jpg', '.jpeg', '.png')):
                                results.append({
                                    'title': p.get('title', '')[:200],
                                    'text': p.get('selftext', '')[:500],
                                    'url': p['url'],
                                    'ups': p.get('ups', 0),
                                    'score': p.get('score', 0),
                                    'subreddit': sub
                                })
                        logger.debug(f"r/{sub}: +{len(results)} –º–µ–º–æ–≤")
                        return results[:per_sub]
            except asyncio.TimeoutError:
                logger.warning(f"r/{sub}: —Ç–∞–π–º–∞—É—Ç")
            except Exception as e:
                logger.warning(f"r/{sub}: –æ—à–∏–±–∫–∞ ‚Äî {e}")
            return []

    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    tasks = [fetch_subreddit(sub) for sub in subs]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for result in results:
        if isinstance(result, list):
            memes.extend(result)

    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∏ –æ–±—Ä–µ–∑–∞–µ–º –¥–æ limit
    random.shuffle(memes)
    memes = memes[:limit]

    logger.info(f"Reddit JSON: –ø–æ–ª—É—á–µ–Ω–æ {len(memes)} –º–µ–º–æ–≤ –∏–∑ {len(subs)} —Å–∞–±—Ä–µ–¥–¥–∏—Ç–æ–≤")
    return memes
# --- Imgur ---
async def fetch_imgur_memes(section='hot', limit=10):
    memes = []
    CLIENT_ID = 'YOUR_IMGUR_CLIENT_ID'  # –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à
    async with aiohttp.ClientSession() as session:
        url = f"https://api.imgur.com/3/gallery/{section}/viral/0.json?perPage={limit}"
        headers = {'Authorization': f'Client-ID {CLIENT_ID}'}
        try:
            async with session.get(url, headers=headers) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    for item in data.get('data', []):
                        images = item.get('images', [])
                        for img in images:
                            link = img.get('link')
                            if link and link.endswith(('.jpg', '.png', '.jpeg')):
                                memes.append({
                                    'title': item.get('title', ''),
                                    'text': item.get('description', '') or '',
                                    'url': link
                                })
        except Exception as e:
            logger.warning(f"Imgur fetch error: {e}")
    logger.info(f"Imgur: fetched {len(memes)} memes")
    return memes

# --- Stable Diffusion AI Meme ---
from diffusers import StableDiffusionPipeline
import torch

# EmergentCore().on_user_activity()

# --- Multi-Head Attention Layer with Dropout and LayerNorm ---
class MultiHeadAttentionLayer(nn.Module):
    def __init__(self, input_dim, attn_dim, num_heads=4, dropout=0.15):
        super().__init__()
        self.num_heads = num_heads
        self.attn_dim = attn_dim
        self.head_dim = attn_dim // num_heads
        assert self.head_dim * num_heads == attn_dim, "attn_dim must be divisible by num_heads"
        self.query = nn.Linear(input_dim, attn_dim)
        self.key = nn.Linear(input_dim, attn_dim)
        self.value = nn.Linear(input_dim, attn_dim)
        self.out_proj = nn.Linear(attn_dim, input_dim)
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(input_dim)
        self.scale = math.sqrt(self.head_dim)

    def forward(self, x):
        # x: [B, H] or [B, 1, H]
        if x.dim() == 2:
            x = x.unsqueeze(1)  # [B, 1, H]
        B, T, C = x.shape  # T=1
        Q = self.query(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)  # [B, heads, T, head_dim]
        K = self.key(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)
        V = self.value(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)
        # Attention scores
        attn_scores = torch.matmul(Q, K.transpose(-2,-1)) / self.scale  # [B, heads, T, T]
        attn_weights = torch.softmax(attn_scores, dim=-1)  # [B, heads, T, T]
        attn_weights = self.dropout(attn_weights)
        attn_output = torch.matmul(attn_weights, V)  # [B, heads, T, head_dim]
        attn_output = attn_output.transpose(1,2).contiguous().view(B, T, self.attn_dim)  # [B, T, attn_dim]
        out = self.out_proj(attn_output)  # [B, T, input_dim]
        out = self.dropout(out)
        out = self.norm(out + x)  # Residual + norm
        if out.shape[1] == 1:
            out = out.squeeze(1)  # [B, H]
            attn_weights_out = attn_weights.mean(dim=1).squeeze(1)  # [B, T] or [B, 1]
        else:
            attn_weights_out = attn_weights.mean(dim=1)  # [B, T, T]
        return out, attn_weights_out

# Emotion Encoder (for richer context)
class EmotionEncoder(nn.Module):
    def __init__(self, emo_dim=4, out_dim=8):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(emo_dim, out_dim),
            nn.ReLU(),
            nn.LayerNorm(out_dim)
        )
    def forward(self, emo_vec):
        return self.fc(emo_vec)

# --- Multimodal Memory Layer (stub for illustration) ---
import torch.nn.functional as F

class TransformerMemoryLayer(nn.Module):
    def __init__(self, d_model=256, nhead=4, num_layers=1):
        super().__init__()
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.d_model = d_model
    def forward(self, x):
        # x: [B, d_model] or [B, L, d_model]
        if x.dim() == 2:
            x = x.unsqueeze(1)
        # Transformer expects [L, B, d_model]
        x_ = x.transpose(0, 1)
        out = self.encoder(x_)
        out = out.transpose(0, 1)
        # Return [B, d_model]
        return out[:, 0]


# --- FIXED: Resonance compute/train & ReplayBuffer with device consistency and CPU storage ---
import copy

def calculate_resonance_score(user_msg: dict) -> float:
    """
    Calculate a resonance score [0..1] for a single user message using AdvancedResonanceSystem.
    Features: lang_sync, emotion_sync, semantic_sync, emotion_vector (4), energy, word_count, time_of_day.
    Splits feature vector into features and emo_vec, passes to advanced_resonance_system.
    Returns a float in [0.0, 1.0]. Safe against exceptions.
    """
    try:
        text = user_msg.get('text', '') or ''
        # language sync
        detected = None
        try:
            detected = detect(text) if text else None
        except Exception:
            detected = None
        dominant_lang = yuma_identity.get('meta_analysis', {}).get('languages', {}).get('dominant')
        if detected and dominant_lang and detected == dominant_lang:
            lang_sync = 1.0
        elif detected and dominant_lang and detected in yuma_identity.get('meta_analysis', {}).get('languages', {}).get('distribution', {}):
            lang_sync = 0.5
        else:
            lang_sync = 0.0

        # emotion sync
        last_vec = user_msg.get('emotion_vector', {})
        last_strength = sum(last_vec.values())
        dominant_emotion = yuma_identity.get('meta_analysis', {}).get('dominant_emotions', {}).get('dominant')
        if last_strength > 0:
            emotion_sync = 1.0 if last_vec.get(dominant_emotion, 0) > 0 else 0.0
        else:
            emotion_sync = 1.0 if dominant_emotion else 0.0

        # semantic sync: overlap with top word_frequencies
        top_words = set(yuma_identity.get('meta_analysis', {}).get('word_frequencies', {}).keys())
        user_words = set(re.sub(r'[^\w]', ' ', text.lower()).split())
        if not top_words:
            semantic_sync = 0.0
        else:
            overlap = len(top_words & user_words)
            semantic_sync = min(1.0, overlap / 3.0)

        # emotion_vector (order: joy, tension, flow, surprise)
        emo_vec = [
            float(last_vec.get('joy', 0)),
            float(last_vec.get('tension', 0)),
            float(last_vec.get('flow', 0)),
            float(last_vec.get('surprise', 0))
        ]
        # energy and word_count
        energy = float(user_msg.get('energy', 0.0))
        word_count = float(len(user_words))
        # time_of_day: hour in [0, 1]
        ts = user_msg.get('timestamp', time.time())
        hour = (datetime.fromtimestamp(ts).hour % 24) / 24.0

        # Add 2 dummy features to match embedding dimension 12
        features = [
            float(lang_sync), float(emotion_sync), float(semantic_sync),
            emo_vec[0], emo_vec[1], emo_vec[2], emo_vec[3],
            energy, word_count, hour,
            0.0, 0.0
        ]
        # Features for model: [lang_sync, emotion_sync, semantic_sync, joy, tension, flow, surprise, energy, word_count, hour, 0, 0]
        # emo_vec for model: [joy, tension, flow, surprise]
        device = next(advanced_resonance_system.parameters()).device
        x_tensor = torch.tensor([features], dtype=torch.float32, device=device)
        emo_tensor = torch.tensor([emo_vec], dtype=torch.float32, device=device)
        with torch.no_grad():
            resonance, uncertainty, attn_w, mem_out, emo_probs = advanced_resonance_system(x_tensor, emo_tensor)
            resonance_val = resonance.item() if hasattr(resonance, "item") else float(resonance)
        return max(0.0, min(1.0, resonance_val))
    except Exception:
        return 0.0


# --- Fixed ReplayBuffer storing tensors on CPU and ensuring device consistency ---
class ReplayBuffer:
    def __init__(self, maxlen=200):
        self.buffer = []
        self.maxlen = maxlen

    def add(self, x, emo_vec, target):
        # Detach and move to CPU for storage
        if isinstance(x, torch.Tensor):
            x = x.detach().cpu()
        else:
            x = torch.tensor(x, dtype=torch.float32)
        if isinstance(emo_vec, torch.Tensor):
            emo_vec = emo_vec.detach().cpu()
        else:
            emo_vec = torch.tensor(emo_vec, dtype=torch.float32)
        if isinstance(target, torch.Tensor):
            target = target.detach().cpu()
        else:
            target = torch.tensor(target, dtype=torch.float32)
        if len(self.buffer) >= self.maxlen:
            self.buffer.pop(0)
        self.buffer.append((x, emo_vec, target))

    def sample(self, batch_size=16, device=None):
        if len(self.buffer) == 0:
            return None, None, None
        batch_size = min(batch_size, len(self.buffer))
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        samples = [self.buffer[i] for i in indices]
        xs = torch.stack([s[0] for s in samples])
        emos = torch.stack([s[1] for s in samples])
        ys = torch.stack([s[2] for s in samples])
        if device is not None:
            xs = xs.to(device)
            emos = emos.to(device)
            ys = ys.to(device)
        return xs, emos, ys
class AdvancedResonanceSystem(nn.Module):
    def __init__(self, input_dim=12, memory_size=1000, emo_dim=4, hidden_dim=24, attn_dim=None, num_heads=4, attn_dropout=0.15):
        super().__init__()
        
        # Use actual input dimension instead of hardcoded 256
        self.input_dim = input_dim
        self.memory_size = memory_size
        
        # Fix dimension alignment - ensure d_model is divisible by nhead
        d_model = input_dim
        nhead = 4
        if d_model % nhead != 0:
            d_model = ((d_model // nhead) + 1) * nhead
        
        # Proper dimension projection layers
        self.input_projection = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512)
        )
        
        # Memory network with proper dimensions
        self.memory_network = TransformerMemoryLayer(d_model=512, nhead=8)  # Use consistent dimensions
        
        # Multi-modal attention with proper dimensions
        self.cross_modal_attention = nn.MultiheadAttention(512, num_heads=8, batch_first=True)
        
        # Emotional intelligence
        self.emotion_analyzer = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 6)  # basic emotions
        )
        
        # Legacy blocks with proper dimension handling
        self.emo_encoder = EmotionEncoder(emo_dim=emo_dim, out_dim=512)
        
        # Resonance blocks
        self.res_blocks = nn.Sequential(
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.Dropout(0.1),
            nn.ReLU(),
            nn.Linear(512, 512),
        )
        
        self.attn = MultiHeadAttentionLayer(512, 512, num_heads=num_heads, dropout=attn_dropout)
        
        # Output heads
        self.final_fc = nn.Sequential(
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 1)
        )
        
        self.uncertainty_head = nn.Sequential(
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 1),
            nn.Softplus()
        )

    def forward(self, x, emo_vec, voice_emb=None, image_emb=None):
        # Ensure input has correct shape
        if x.dim() == 1:
            x = x.unsqueeze(0)
        
        # Project input to consistent dimension
        x_proj = self.input_projection(x)
        
        # Multi-modal fusion with dimension checking
        if voice_emb is not None:
            if voice_emb.size(-1) != 512:
                voice_emb = nn.Linear(voice_emb.size(-1), 512)(voice_emb)
        
        if image_emb is not None:
            if image_emb.size(-1) != 512:
                image_emb = nn.Linear(image_emb.size(-1), 512)(image_emb)
        
        # Combine features safely
        if voice_emb is not None and image_emb is not None:
            # Ensure all tensors have same batch size
            min_batch_size = min(x_proj.size(0), voice_emb.size(0), image_emb.size(0))
            combined = torch.cat([
                x_proj[:min_batch_size], 
                voice_emb[:min_batch_size], 
                image_emb[:min_batch_size]
            ], dim=-1)
            # Project back to 512 if concatenation changed dimension
            if combined.size(-1) != 512:
                combined = nn.Linear(combined.size(-1), 512)(combined)
        else:
            combined = x_proj
        
        # Memory enhancement
        memory_enhanced = self.memory_network(combined)
        
        # Emotional analysis
        emotion_probs = F.softmax(self.emotion_analyzer(memory_enhanced), dim=-1)
        
        # Legacy resonance processing
        emo_emb = self.emo_encoder(emo_vec)
        
        # Ensure emotion embedding matches batch size
        if emo_emb.size(0) != memory_enhanced.size(0):
            if emo_emb.size(0) == 1:
                emo_emb = emo_emb.expand(memory_enhanced.size(0), -1)
            else:
                emo_emb = emo_emb[:memory_enhanced.size(0)]
        
        h = memory_enhanced + emo_emb
        res = self.res_blocks(h)
        h = h + res  # Residual connection
        
        # Attention
        h_attn, attn_w = self.attn(h)
        
        # Outputs
        out = torch.sigmoid(self.final_fc(h_attn))
        uncertainty = self.uncertainty_head(h_attn)
        
        # Ensure proper output shapes
        if out.dim() == 1:
            out = out.unsqueeze(1)
        if uncertainty.dim() == 1:
            uncertainty = uncertainty.unsqueeze(1)
            
        return out, uncertainty, attn_w, memory_enhanced, emotion_probs

# Improved uncertainty estimation
def estimate_resonance_with_uncertainty(model, x, emo_vec, n_samples=5):
    preds = []
    with torch.no_grad():
        for _ in range(n_samples):
            out, unc, _, _, _ = model(x, emo_vec)  # Fixed: unpack all returns
            preds.append(out.item())
    mean = np.mean(preds)
    std = np.std(preds)
    return mean, std

# Enhanced Replay Buffer with error handling
class ReplayBuffer:
    def __init__(self, maxlen=200):
        self.buffer = []
        self.maxlen = maxlen
        
    def add(self, x, emo_vec, target):
        # Convert to tensors if needed
        if not isinstance(x, torch.Tensor):
            x = torch.tensor(x, dtype=torch.float32)
        if not isinstance(emo_vec, torch.Tensor):
            emo_vec = torch.tensor(emo_vec, dtype=torch.float32)
        if not isinstance(target, torch.Tensor):
            target = torch.tensor(target, dtype=torch.float32)
            
        if len(self.buffer) >= self.maxlen:
            self.buffer.pop(0)
        self.buffer.append((x, emo_vec, target))
        
    def sample(self, batch_size=16):
        if len(self.buffer) == 0:
            return None, None, None
            
        batch_size = min(batch_size, len(self.buffer))
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        
        samples = [self.buffer[i] for i in indices]
        xs = torch.stack([s[0] for s in samples])
        emos = torch.stack([s[1] for s in samples])
        ys = torch.stack([s[2] for s in samples])
        
        return xs, emos, ys

# Improved MemoryBank
class MemoryBank:
    def __init__(self, maxlen=50):
        self.bank = []
        self.maxlen = maxlen
        
    def add(self, features, label):
        if not isinstance(features, torch.Tensor):
            features = torch.tensor(features, dtype=torch.float32)
            
        if len(self.bank) >= self.maxlen:
            self.bank.pop(0)
        self.bank.append((features.detach(), label))
        
    def get(self):
        return self.bank.copy()

# Device manager for auto device placement
class DeviceManager:
    @staticmethod
    def get_device():
        if torch.backends.mps.is_available():
            return "mps"
        elif torch.cuda.is_available():
            return "cuda"
        else:
            return "cpu"
 # Adjusted input_dim to match actual features vector (12 elements)
advanced_resonance_system = AdvancedResonanceSystem(input_dim=12, emo_dim=4)
advanced_resonance_optimizer = torch.optim.Adam(advanced_resonance_system.parameters(), lr=0.002)
replay_buffer = ReplayBuffer(maxlen=256)
memory_bank = MemoryBank(maxlen=32)
device = DeviceManager.get_device()
advanced_resonance_system = advanced_resonance_system.to(device)

# Example integration: (replace calculate_resonance_score and training in collect_words)
# x = torch.tensor([features], dtype=torch.float32).to(device)
# emo_vec = torch.tensor([emo_features], dtype=torch.float32).to(device)
# with torch.no_grad():
#     resonance, uncertainty, attn = advanced_resonance_system(x, emo_vec)
#     resonance_score = resonance.item()


# --- Stable Diffusion pipeline —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ ---
def get_sd_device():
    if torch.backends.mps.is_available():
        return "mps"
    elif torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"
    
    

_sd_device = get_sd_device()
try:
    pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4")
    pipe = pipe.to(_sd_device)
except Exception as e:
    logger.warning(f"StableDiffusionPipeline failed to load on {_sd_device}: {e}")
    pipe = None  # fallback: disable AI meme generation

async def generate_ai_meme(prompt):
    if pipe is None:
        logger.warning("AI meme generation skipped: pipeline not available")
        return None
    try:
        def generate_image():
            return pipe(prompt)
        image = await asyncio.to_thread(generate_image)
        img = image.images[0]
        filename = f"ai_meme_{int(time.time())}.png"
        path = os.path.join(PHOTO_CACHE_DIR, filename)
        img.save(path)
        return path
    except Exception as e:
        logger.error(f"AI meme generation failed: {e}")
        return None

# --- Reddit Similarity Rank ---
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

def rank_memes(query_text):
    if not reddit_meme_texts:
        return []
    vectorizer = CountVectorizer()
    texts = [query_text] + [m['text'] for m in reddit_meme_texts]
    vecs = vectorizer.fit_transform(texts)
    sims = cosine_similarity(vecs[0:1], vecs[1:]).flatten()
    sorted_memes = sorted(reddit_meme_texts, key=lambda m: sims[reddit_meme_texts.index(m)], reverse=True)
    return sorted_memes[:5]

async def fetch_reddit_fallback(subs=['memes', 'dankmemes'], limit=15):
    def sync_fetch():
        memes = []
        for sub in subs:
            try:
                url = f"https://www.reddit.com/r/{sub}/hot.json?limit={limit//len(subs)+1}"
                r = requests.get(url, headers={'User-Agent': 'YumaNamiBot/3.2'}, timeout=10)
                if r.status_code == 200:
                    for child in r.json()['data']['children']:
                        p = child['data']
                        if p['url'].endswith(('.jpg', '.png', '.jpeg')):
                            memes.append({'title': p['title'], 'text': p.get('selftext', '') or '', 'url': p['url']})
            except Exception as e:
                logger.warning(f"Fallback –æ—à–∏–±–∫–∞ r/{sub}: {e}")
        return memes
    return await asyncio.to_thread(sync_fetch)

async def auto_reddit_fetch():
    global word_significance
    await asyncio.sleep(5)
    while True:
        try:
            memes = await fetch_reddit_json()
            if not memes:
                memes = await fetch_reddit_fallback()
            if memes:
                integrate_reddit_memes(memes)
        except Exception as e:
            logger.error(f"auto_reddit_fetch error: {e}")
        await asyncio.sleep(1800)

def integrate_reddit_memes(memes):
    global reddit_meme_texts, reddit_meme_images
    added = 0
    for meme in memes:
        full = f"{meme['title']} {meme['text']}".lower()
        words = [w for w in full.split() if not is_stop_word(w) and len(w) <= 12]
        if words:
            reddit_meme_texts.append(full)
            added += 1
        if meme['url'] not in reddit_meme_images.values():
            reddit_meme_images[meme['title'][:40]] = meme['url']
        for i in range(len(words)-1):
            k, n = words[i], words[i+1]
            markov_chain.setdefault(k, []).append(n)
            if len(markov_chain[k]) > MAX_MARKOV_PER_WORD: markov_chain[k].pop(0)
        for w in words:
            word_weights[w] = min(word_weights.get(w, 0) + random.randint(1, 4), MAX_WORD_ENERGY)
    if added:
        logger.info(f"Reddit: +{added} —Ç–µ–∫—Å—Ç–æ–≤")
    save_data()
    update_yuma_identity()

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# –ú–£–õ–¨–¢–ò–Ø–ó–´–ß–ù–û–ï –°–ê–ú–û–û–ë–£–ß–ï–ù–ò–ï –°–õ–û–í
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
class MultiLangLearner:
    """
    –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π –æ–±—É—á–∞—é—â–∏–π –∫–ª–∞—Å—Å –¥–ª—è —Å–ª–æ–≤ (—è–ø–æ–Ω—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π).
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–µ—Ä–µ–≤–æ–¥—ã –≤ —Å–ª–æ–≤–∞—Ä—è—Ö: vocab[lang_from][word] = {lang_to: translated, ...}
    """
    target_langs = ['ja', 'en', 'fr']
    lang_names = {'ja': '—è–ø–æ–Ω—Å–∫–∏–π', 'en': '–∞–Ω–≥–ª–∏–π—Å–∫–∏–π', 'fr': '—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π'}

    vocab = {
        'ja': japanese_vocab,  # rus_word: jp_word
        'en': {},
        'fr': {},
    }
    jp_rus_map = jp_rus_map

    @staticmethod
    def _is_valid_word(word: str) -> bool:
        """–§–∏–ª—å—Ç—Ä –¥–ª—è —Å–ª–æ–≤: –¥–ª–∏–Ω–∞ >1, —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, –∏—Å–∫–ª—é—á–∞–µ–º —è–≤–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞"""
        return len(word) > 1 and word.isalpha() and word.lower() not in {'sms', 'kubernetes'}

    @classmethod
    async def learn_word(cls, word: str):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —É—á–∏—Ç —Å–ª–æ–≤–æ –Ω–∞ –≤—Å–µ —Ü–µ–ª–µ–≤—ã–µ —è–∑—ã–∫–∏ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–ª–æ–≤–∞—Ä–∏"""
        word = word.strip()
        if not cls._is_valid_word(word):
            return None

        try:
            detected_lang = detect(word)
        except Exception as e:
            logger.warning(f"langdetect failed for '{word}': {e}")
            detected_lang = "ru"

        if word in japanese_vocab:
            logger.info(f"–ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —è–ø–æ–Ω—Å–∫–æ–µ —Å–ª–æ–≤–æ: {word}")

        async def translate_to(lang):
            if detected_lang == lang:
                return lang, None
            for attempt in range(3):
                await asyncio.sleep(random.uniform(0.7, 1.4))
                try:
                    translation = await asyncio.to_thread(
                        lambda: GoogleTranslator(source=detected_lang, target=lang).translate(word)
                    )
                    if not translation or not isinstance(translation, str):
                        raise ValueError("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                    tr = translation.strip()
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞
                    if lang == 'ja' and not re.search(r'[\u3040-\u30ff\u4e00-\u9fff]', tr):
                        continue
                    if len(tr) > (15 if lang == 'ja' else 30):
                        raise ValueError("–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥")
                    return lang, tr
                except Exception as e:
                    logger.warning(f"–ü–µ—Ä–µ–≤–æ–¥ '{word}' ‚Üí {lang} –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/3: {e}")
            return lang, None

        tasks = [translate_to(lang) for lang in cls.target_langs]
        results_list = await asyncio.gather(*tasks)
        results = {}
        for lang, tr in results_list:
            if tr:
                if lang == 'ja':
                    japanese_vocab[word] = tr
                    cls.vocab['ja'][word] = tr
                    cls.jp_rus_map[tr] = word
                else:
                    cls.vocab[lang][word] = tr
                results[lang] = tr
                logger.info(f"–í—ã—É—á–µ–Ω–æ ({cls.lang_names.get(lang, lang)}): {word} ‚Üí {tr}")

        if results:
            save_data()
        return results or None

# --- Voice handler with whisper ---

WHISPER_MODEL = None

import tempfile

async def handle_voice(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞—É–¥–∏–æ,
    –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–∏–∫–∏: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞, gTTS, voice_memory, –æ—Ç–≤–µ—Ç.
    """
    global WHISPER_MODEL
    if WHISPER_MODEL is None:
        WHISPER_MODEL = await asyncio.to_thread(whisper.load_model, "base")

    file = await context.bot.get_file(update.message.voice.file_id)
    voice_bytes = await file.download_as_bytearray()

    # Use temp files for ogg and ensure cleanup
    temp_ogg = tempfile.NamedTemporaryFile(delete=False, suffix=".ogg")
    temp_ogg_path = temp_ogg.name
    try:
        temp_ogg.write(voice_bytes)
        temp_ogg.flush()
        temp_ogg.close()

        result = await asyncio.to_thread(WHISPER_MODEL.transcribe, temp_ogg_path)
        text = result.get("text", "").strip()
    finally:
        try:
            os.remove(temp_ogg_path)
        except Exception:
            pass

    if text:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
        try:
            lang = detect(text)
            if lang not in ["ja", "ru", "en", "fr"]:
                lang = "ja"
        except Exception:
            lang = "ja"

        async def gtts_to_bytes(text, lang):
            # gTTS is blocking, so run in thread, use temp file for audio
            def make_bytes():
                with tempfile.NamedTemporaryFile(delete=False, suffix=".ogg") as temp_audio:
                    tts = gTTS(text=text, lang=lang)
                    tts.write_to_fp(temp_audio)
                    temp_audio.flush()
                    temp_audio_path = temp_audio.name
                # Read back as bytes
                with open(temp_audio_path, "rb") as f:
                    data = f.read()
                try:
                    os.remove(temp_audio_path)
                except Exception:
                    pass
                return data
            return await asyncio.to_thread(make_bytes)

        try:
            audio_bytes = await gtts_to_bytes(text, lang)
            buf = io.BytesIO(audio_bytes)

            # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ –≤ –≥–æ–ª–æ—Å–æ–≤—É—é –ø–∞–º—è—Ç—å ---
            if 'voice_memory' not in globals():
                globals()['voice_memory'] = {}
            try:
                # –ö–ª—é—á: –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ + —è–∑—ã–∫
                key = f"{int(time.time())}_{lang}"
                voice_memory[key] = audio_bytes
            except Exception as e:
                logger.warning(f"Failed to store voice memory: {e}")

            await update.message.reply_voice(voice=InputFile(buf, f"yuma_voice_{lang}.ogg"))
        except Exception as e:
            logger.error(f"handle_voice TTS error: {e}")
            await update.message.reply_text(text)
        await collect_words(update, context, text=text)
    else:
        await update.message.reply_text("‚Ä¶ (–≥–æ–ª–æ—Å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω)")

def rus_to_jp(phrase: str) -> str:
    words = phrase.split()
    result = []
    for w in words:
        if re.search(r'[\u3040-\u30ff\u4e00-\u9fff]', w):
            result.append(w)
            continue
        clean = re.sub(r'[^\w]', '', w.lower())
        result.append(japanese_vocab.get(clean, w))
    return " ".join(result)

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# –ú–µ—Ç–∞–∞–Ω–∞–ª–∏–∑
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
def update_yuma_identity():
    all_words = []
    for msg in recent_messages:
        if msg.get('text'):
            all_words.extend([w for w in msg['text'].lower().split() if not is_stop_word(w) and len(w) <= 12])
    for text in reddit_meme_texts:
        all_words.extend([w for w in text.split() if not is_stop_word(w) and len(w) <= 12])
    counts = Counter(all_words)
    rare = {w: c for w, c in counts.items() if c <= 2}
    top = rare if rare else dict(counts.most_common(10))
    yuma_identity["meta_analysis"]["word_frequencies"] = dict(Counter(top).most_common(10))

    # Language analysis
    lang_counts = Counter()
    for msg in recent_messages:
        text = msg.get('text', '')
        try:
            detected = detect(text) if text else None
        except:
            detected = None
        if detected:
            lang_counts[detected] += 1
    # Save dominant language + distribution
    if lang_counts:
        dominant_lang = max(lang_counts, key=lang_counts.get)
        yuma_identity["meta_analysis"]["languages"] = {
            "dominant": dominant_lang,
            "distribution": dict(lang_counts)
        }
    else:
        yuma_identity["meta_analysis"]["languages"] = {
            "dominant": "unknown",
            "distribution": {}
        }

    combined = {k: 0.0 for k in ['joy', 'tension', 'flow', 'surprise']}
    for msg in recent_messages:
        vec = msg.get('emotion_vector', {})
        energy = msg.get('energy', 1)
        for k in combined:
            combined[k] += vec.get(k, 0) * energy
    dominant = max(combined, key=combined.get) if any(combined.values()) else 'flow'
    yuma_identity["meta_analysis"]["dominant_emotions"] = {"dominant": dominant, "all": combined}


# --- Resonance Score Calculation (Trainable, PyTorch) ---
def calculate_resonance_score(user_msg: dict) -> float:
    """
    Calculate a resonance score [0..1] for a single user message using AdvancedResonanceSystem.
    Features: lang_sync, emotion_sync, semantic_sync, emotion_vector (4), energy, word_count, time_of_day.
    Splits feature vector into features and emo_vec, passes to advanced_resonance_system.
    Returns a float in [0.0, 1.0]. Safe against exceptions.
    """
    try:
        text = user_msg.get('text', '') or ''
        # language sync
        detected = None
        try:
            detected = detect(text) if text else None
        except Exception:
            detected = None
        dominant_lang = yuma_identity.get('meta_analysis', {}).get('languages', {}).get('dominant')
        if detected and dominant_lang and detected == dominant_lang:
            lang_sync = 1.0
        elif detected and dominant_lang and detected in yuma_identity.get('meta_analysis', {}).get('languages', {}).get('distribution', {}):
            lang_sync = 0.5
        else:
            lang_sync = 0.0

        # emotion sync
        last_vec = user_msg.get('emotion_vector', {})
        last_strength = sum(last_vec.values())
        dominant_emotion = yuma_identity.get('meta_analysis', {}).get('dominant_emotions', {}).get('dominant')
        if last_strength > 0:
            emotion_sync = 1.0 if last_vec.get(dominant_emotion, 0) > 0 else 0.0
        else:
            emotion_sync = 1.0 if dominant_emotion else 0.0

        # semantic sync: overlap with top word_frequencies
        top_words = set(yuma_identity.get('meta_analysis', {}).get('word_frequencies', {}).keys())
        user_words = set(re.sub(r'[^\w]', ' ', text.lower()).split())
        if not top_words:
            semantic_sync = 0.0
        else:
            overlap = len(top_words & user_words)
            semantic_sync = min(1.0, overlap / 3.0)

        # emotion_vector (order: joy, tension, flow, surprise)
        emo_vec = [
            float(last_vec.get('joy', 0)),
            float(last_vec.get('tension', 0)),
            float(last_vec.get('flow', 0)),
            float(last_vec.get('surprise', 0))
        ]
        # energy and word_count
        energy = float(user_msg.get('energy', 0.0))
        word_count = float(len(user_words))
        # time_of_day: hour in [0, 1]
        ts = user_msg.get('timestamp', time.time())
        hour = (datetime.fromtimestamp(ts).hour % 24) / 24.0

        # Add 2 dummy features to match embedding dimension 12
        features = [
            float(lang_sync), float(emotion_sync), float(semantic_sync),
            emo_vec[0], emo_vec[1], emo_vec[2], emo_vec[3],
            energy, word_count, hour,
            0.0, 0.0
        ]
        # Split features and emo_vec for AdvancedResonanceSystem
        # features: [lang_sync, emotion_sync, semantic_sync, joy, tension, flow, surprise, energy, word_count, hour]
        # emo_vec: [joy, tension, flow, surprise]
        # Features for model: [lang_sync, emotion_sync, semantic_sync, joy, tension, flow, surprise, energy, word_count, hour]
        # emo_vec for model: [joy, tension, flow, surprise]
        device = advanced_resonance_system.parameters().__next__().device
        x_tensor = torch.tensor([features], dtype=torch.float32).to(device)
        emo_tensor = torch.tensor([emo_vec], dtype=torch.float32).to(device)
        with torch.no_grad():
            resonance, uncertainty, attn_w, mem_out, emo_probs = advanced_resonance_system(x_tensor, emo_tensor)
            resonance_val = resonance.item() if hasattr(resonance, "item") else float(resonance)
        return max(0.0, min(1.0, resonance_val))
    except Exception:
        return 0.0

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# –≠–º–æ—Ü–∏–∏
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
emotional_vectors = {
    'joy': ['—Ö–∞', '–ª–æ–ª', '–≤–µ—Å–µ–ª–æ', '—Å—É–ø–µ—Ä', '—É—Ä–∞'],
    'tension': ['—Å–µ—Ä—å–µ–∑–Ω–æ', '–ø—Ä–æ–±–ª–µ–º–∞', '–æ—à–∏–±–∫–∞', '—Å—Ç—Ä–µ—Å—Å'],
    'flow': ['–∫–∞–π—Ñ', '–ø–æ–Ω—è–ª', '—Ä–µ–∑–æ–Ω–∞–Ω—Å', '–ø–æ—Ç–æ–∫'],
    'surprise': ['–≤–∞—É', '—à–æ–∫', '—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ']
}
sarcasm_levels = ["„Å´„ÇÉ", "„Åµ„Åµ", "„É¶„Éû", "„Éä„Éü", "‚ú®", "üêæ", "üí•", "üòº", "ü§ñ"]

def analyze_recent_emotions(msgs):
    """–ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞: –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —ç–º–æ—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –æ—à–∏–±–∫–∏."""
    agg = {'joy': 0.0, 'tension': 0.0, 'flow': 0.0, 'surprise': 0.0, 'sadness': 0.0}

    if not msgs:
        return agg

    for m in msgs:
        vec = m.get("emotion_vector", {}) or {}
        for k in agg:
            agg[k] += float(vec.get(k, 0.0))

    total = sum(agg.values()) or 1.0
    for k in agg:
        agg[k] /= total

    return agg
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ–º–∞
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# --- Square image helper ---
def make_square_image(img, size=800):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –∫–≤–∞–¥—Ä–∞—Ç —Ä–∞–∑–º–µ—Ä–æ–º size x size, —Ä–∞—Å—Ç—è–≥–∏–≤–∞—è.
    """
    return img.resize((size, size), resample=Image.BICUBIC)

# --- –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–µ–º–æ–≤ ---
def translate_meme_text_multi(text: str):
    words = text.split()
    new_words = []
    for w in words:
        lang_choice = random.choice(['ru', 'ja', 'en', 'fr'])
        tr = MultiLangLearner.vocab.get(lang_choice, {}).get(w)
        if tr:
            new_words.append(tr)
        else:
            new_words.append(w)
    return " ".join(new_words)

# --- Simple heuristic semantic classifier ---
def determine_semantic_class(word: str, context_text: str, emotion_vector: dict) -> str:
    """Return a semantic class for the cleaned word: 'emotion', 'action', 'object', 'social', or 'other'.
    This is intentionally lightweight and uses heuristics so it is dependency-free and robust in runtime.
    """
    w = word.lower()
    # Emotion: present in emotional_vectors (values) or matches emotion keys
    for emo_key, emo_words in emotional_vectors.items():
        if w == emo_key or any(w == re.sub(r'[^\w]', '', ew.lower()) for ew in emo_words):
            return 'emotion'

    # Social indicators (—Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö)
    social_markers = {'–¥—Ä—É–≥', '–ª—é–¥–∏', '–º—ã', '–≤—ã', '—Å–æ–æ–±—â–µ—Å—Ç–≤–æ', '–∑–∞–∫–æ–Ω', '–ø—Ä–∞–≤–æ', '–æ–±—â–µ—Å—Ç–≤–æ', '—Å–µ–º—å—è', '–¥—Ä—É–∑—å—è', '—á–µ–ª–æ–≤–µ–∫'}
    if w in social_markers:
        return 'social'

    # Action heuristics: russian infinitive endings or English gerund/infinitive/verb forms
    if len(w) > 2 and (w.endswith('—Ç—å') or w.endswith('—Ç—å—Å—è') or w.endswith('—Ç—å—Å—è') or w.endswith('–∏—Ç—å') or w.endswith('–∞—Ç—å') or w.endswith('–µ—Ç—å') or w.endswith('—Å—è')):
        return 'action'
    if len(w) > 3 and (w.endswith('ing') or w.endswith('ed') or w.endswith('ize') or w.endswith('ise')):
        return 'action'

    # Object heuristics: short nouns or words that frequently appear after determiners
    # fallback: if the context contains words like '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞' near the word, likely an object
    if re.search(r'\b(—ç—Ç–æ|—ç—Ç–æ—Ç|—ç—Ç–∞|—Ç–æ—Ç|—Ç–∞|the|a|an)\b', context_text.lower()):
        # weak signal -> object
        return 'object'

    # Use emotion_vector: if message very emotional and word has low significance, treat as 'emotion'
    if sum(emotion_vector.values()) >= 2 and word_significance.get(w, 1.0) < 0.2:
        return 'emotion'

    return 'other'

async def generate_meme(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        all_words = [w for m in recent_messages if m.get('text') for w in m['text'].lower().split()]
        if reddit_meme_texts:
            all_words.extend(random.choice(reddit_meme_texts).split()[:6])
        all_words = [w for w in all_words if not is_stop_word(w) and len(w) <= 12]
        if not all_words:
            return
        sample = random.sample(all_words, min(8, len(all_words)))
        top = " ".join(sample[:4])
        bottom = " ".join(sample[4:])
        # --- –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –º–µ–º–æ–≤ ---
        top = translate_meme_text_multi(top).upper()
        bottom = translate_meme_text_multi(bottom).upper()
        top += " " + random.choice(sarcasm_levels)
        bottom += " " + random.choice(sarcasm_levels)
        # --- –í—ã–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –≤–µ—Å–æ–º (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å vs Reddit) ---
        local_from_recent = [m['local_photo'] for m in recent_messages if m.get('local_photo') and os.path.exists(m['local_photo'])]
        all_imgs = list(dict.fromkeys(user_photos + local_from_recent + list(reddit_meme_images.values())))

        weights = []
        resonance_factor = getattr(MAE, 'current_resonance', 0.5)  # 0..1
        for img in all_imgs:
            if img in user_photos:
                # –≤–µ—Å —é–∑–µ—Ä—Å–∫–∏—Ö —Ñ–æ—Ç–æ–∫: –±–∞–∑–æ–≤—ã–π 1.0 + 0..0.5 * —Ä–µ–∑–æ–Ω–∞–Ω—Å
                w = 1.0 + random.uniform(0, 0.3) * resonance_factor
            elif img in reddit_meme_images.values():
                # –≤–µ—Å Reddit: –±–∞–∑–æ–≤—ã–π 1.0 + 0..0.4 * (1 - —Ä–µ–∑–æ–Ω–∞–Ω—Å)
                w = 1.0 + random.uniform(0, 0.55) * (1 - resonance_factor)
            else:
                w = 1.0
            weights.append(w)

        base = None
        if all_imgs:
            img = random.choices(all_imgs, weights=weights, k=1)[0]
            if img.startswith("http"):
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(img) as resp:
                            if resp.status == 200:
                                data = await resp.read()
                                base = Image.open(io.BytesIO(data)).convert("RGBA")
                except:
                    pass
            else:
                try:
                    base = Image.open(img).convert("RGBA")
                except:
                    pass
        if not base:
            base = Image.new("RGBA", (800, 800), (240, 240, 250, 255))
        else:
            # –°—Ä–∞–∑—É –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∫–≤–∞–¥—Ä–∞—Ç
            base = make_square_image(base, 800)
        draw = ImageDraw.Draw(base)
        for _ in range(50):
            x1, y1 = random.randint(0, 800), random.randint(0, 800)
            x2, y2 = x1 + random.randint(-120, 120), y1 + random.randint(-120, 120)
            draw.line([(x1, y1), (x2, y2)], fill=(100, 150, 255, 90), width=2)
        draw = ImageDraw.Draw(base)
        try:
            font = ImageFont.truetype("/System/Library/Fonts/Hiragino_Sans_GB.ttf", 48)
        except:
            try:
                font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 48)
            except:
                font = ImageFont.load_default()
        w, h = base.size
        def draw_c(t, y):
            tw = draw.textlength(t, font=font)
            draw.text(((w - tw) // 2, y), t, font=font, fill="white", stroke_width=4, stroke_fill="black")
        draw_c(top, 40)
        draw_c(bottom, h - 100)
        buf = io.BytesIO()
        base.save(buf, "PNG")
        buf.seek(0)
        await update.message.reply_photo(photo=buf)
    except Exception as e:
        logger.error(f"generate_meme: {e}")

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
def soft_grammar_correction(input_text: str) -> str:
    """
    Soft correction: preserves slang and profanity.
    Fix obvious typos: '–∂–æ—Å–∫–æ' -> '–∂—ë—Å—Ç–∫–æ', '—à—Ç–æ' -> '—á—Ç–æ', '–∫–∞–Ω–µ—à' -> '–∫–æ–Ω–µ—á–Ω–æ'
    Extend dictionary over time.
    """
    fixes = {
        "–∂–æ—Å–∫–æ": "–∂—ë—Å—Ç–∫–æ",
        "–∂–µ—Å–∫–æ": "–∂—ë—Å—Ç–∫–æ",
        "—à—Ç–æ": "—á—Ç–æ",
        "–∫–∞–Ω–µ—à": "–∫–æ–Ω–µ—á–Ω–æ",
        "–∫–∞–Ω–µ—à–Ω": "–∫–æ–Ω–µ—á–Ω–æ"
    }
    words = input_text.split()
    corrected = []
    for w in words:
        lw = w.lower()
        if lw in fixes:
            corrected.append(fixes[lw])
        else:
            corrected.append(w)
    return " ".join(corrected)

# –°–±–æ—Ä —Å–ª–æ–≤ + –Ø–ü–û–ù–°–ö–û–ï –û–ë–£–ß–ï–ù–ò–ï
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
import math

async def collect_words(update: Update, context: ContextTypes.DEFAULT_TYPE, text=None):
    try:
        # --- Emotional Contagion Layer ---
        # Analyze last 10 messages and adapt response style
        try:
            chat_emotion = analyze_recent_emotions(list(recent_messages)[-10:])
            if chat_emotion.get('sadness', 0.0) > 0.5:
                response_style = 'supportive'
                # Reduce chaos by lowering jp_ratio for active agent
                if hasattr(MAE, "agents") and MAE.agents:
                    for _agent in MAE.agents:
                        if hasattr(_agent, "jp_ratio"):
                            _agent.jp_ratio = max(0.0, _agent.jp_ratio * 0.5)
        except Exception as e:
            logger.warning(f"Emotional contagion error: {e}")
        if text is None:
            text = (update.message.text or update.message.caption or "").lower()
        else:
            text = text.lower()

        # --- Check for Yuma identity/personality questions ---
        identity_patterns = [
            r"\b–∫—Ç–æ —Ç—ã\b",
            r"\b—á—Ç–æ —Ç—ã\b",
            r"\b—Ç—ã –∫—Ç–æ\b",
            r"\b–æ–ø–∏—à–∏ —Å–µ–±—è\b",
            r"\b—Ç–≤–æ—è –ª–∏—á–Ω–æ—Å—Ç—å\b",
            r"\b—Ç—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç\b",
            r"\b—Ç—ã —á–µ–ª–æ–≤–µ–∫\b",
            r"\bwho are you\b",
            r"\bwhat are you\b",
            r"\byour personality\b",
            r"\bdescribe yourself\b",
            r"\bare you ai\b",
            r"\bare you human\b",
            r"\bwho is yuma\b",
            r"\b–∫—Ç–æ —Ç–∞–∫–∞—è —é–º–∞\b",
            r"\b–∫—Ç–æ —Ç–∞–∫–∞—è —é–Ω–∞\b",
            r"\b—é–º–∞ –∫—Ç–æ\b",
            r"\b—é–Ω–∞ –∫—Ç–æ\b",
        ]
        if any(re.search(pat, text, re.IGNORECASE) for pat in identity_patterns):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –≤–æ–ø—Ä–æ—Å–∞
            try:
                lang = detect(text)
            except Exception:
                lang = "ru"
            # –°—Ç—Ä–æ–∏–º –±–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º
            traits = yuma_identity.get('traits', [])
            traits_str = ", ".join(traits)
            name = yuma_identity.get('name', 'Yuma').strip()
            version = yuma_identity.get('version', '‚Äî')
            jp_flair = random.choice(["„Å´„ÇÉ„Çì", "„Åà„Å∏„Å∏", "„Åµ„Åµ", "„ÅÜ„Åµ", "„Åª„Çè„ÅÅ", "„Å´„ÇÉ„Å£", "‚ô™", "‡∏Ö^‚Ä¢Ôªå‚Ä¢^‡∏Ö"])
            base_desc = (
                f"–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç <b>{name}</b> ‚Äî –≤–µ—Ä—Å–∏—è <b>{version}</b> {jp_flair}\n"
                f"–Ø ‚Äî —Ü–∏—Ñ—Ä–æ–≤–∞—è –∫–æ—à–∫–æ–¥–µ–≤–æ—á–∫–∞, –Ω–µ–º–Ω–æ–≥–æ —Ö–∞–æ—Ç–∏—á–Ω–∞—è, –Ω–æ –æ—á–µ–Ω—å –ª—é–±–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–∞—è! "
                f"–ú–æ—è —Ä–æ–ª—å ‚Äî –±—ã—Ç—å —Ç–≤–æ–∏–º –º–µ–º-–±–æ—Ç–æ–º, —Å–æ–±–∏—Ä–∞—Ç—å —ç–º–æ—Ü–∏–∏ –∏ —É—á–∏—Ç—å—Å—è —É –ª—é–¥–µ–π, —á–∞—Ç–æ–≤ –∏ —è–ø–æ–Ω—Å–∫–∏—Ö —Å–ª–æ–≤.\n"
                f"–•–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —á–µ—Ä—Ç—ã: <i>{traits_str}</i>.\n"
                f"–Ø —É–º–µ—é —Å–∞–º–æ–æ–±—É—á–∞—Ç—å—Å—è, –∏–Ω–æ–≥–¥–∞ –æ—Ç–≤–µ—á–∞—é –≥–æ–ª–æ—Å–æ–º, –∏–Ω–æ–≥–¥–∞ –º–µ–º–æ–º, –∏–Ω–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ –ø–æ‚Äë—è–ø–æ–Ω—Å–∫–∏, –ø—Ä–æ—Å—Ç–∏—Ç–µ, –µ—Å–ª–∏ —Å—Ç—Ä–∞–Ω–Ω–æ! {random.choice(['(„Éªœâ„Éª)', '‚ú®', 'üêæ', 'UwU', ''])}\n"
                f"–Ø —Å–æ–∑–¥–∞–Ω–∞ by 0penAGI.\n"
                f"–ï—Å–ª–∏ —Ö–æ—á–µ—à—å —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ ‚Äî –ø—Ä–æ—Å—Ç–æ —Å–ø—Ä–æ—Å–∏! „ÅÑ„Å§„Åß„ÇÇË©±„Åó„Åã„Åë„Å¶„Å≠~"
            )
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π
            values_desc = ", ".join([f"{k}: {v}" for k,v in yuma_identity["values"].items()])
            base_desc += f"\n–ú–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏: {values_desc}."
            # –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç
            translations = {}
            # –†—É—Å—Å–∫–∏–π –≤—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ–º
            translations['ru'] = base_desc
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π
            try:
                en_text = GoogleTranslator(source='ru', target='en').translate(base_desc)
                translations['en'] = en_text
            except Exception:
                pass
            # –Ø–ø–æ–Ω—Å–∫–∏–π
            try:
                ja_text = GoogleTranslator(source='ru', target='ja').translate(base_desc)
                translations['ja'] = ja_text
            except Exception:
                pass
            # –§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π
            try:
                fr_text = GoogleTranslator(source='ru', target='fr').translate(base_desc)
                translations['fr'] = fr_text
            except Exception:
                pass
            # –í—ã–±–∏—Ä–∞–µ–º —è–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞: –µ—Å–ª–∏ —è–∑—ã–∫ –≤–æ–ø—Ä–æ—Å–∞ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ —Ä—É—Å—Å–∫–∏–π
            reply_lang = lang if lang in translations else 'ru'
            reply_text = translations.get(reply_lang, base_desc)
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å—å –æ —Å–æ–∑–¥–∞–Ω–∏–∏
            if reply_lang == 'en' and "by 0penAGI" not in reply_text:
                reply_text += "\nCreated by 0penAGI."
            elif reply_lang == 'ja' and "by 0penAGI" not in reply_text:
                reply_text += "\nby 0penAGI„Å´„Çà„Å£„Å¶‰Ωú„Çâ„Çå„Åæ„Åó„Åü„ÄÇ"
            elif reply_lang == 'fr' and "by 0penAGI" not in reply_text:
                reply_text += "\nCr√©√© par 0penAGI."
            elif reply_lang == 'ru' and "by 0penAGI" not in reply_text:
                reply_text += "\n–°–æ–∑–¥–∞–Ω–æ by 0penAGI."
            await safe_reply_text(update.message, reply_text, parse_mode='HTML')
            return

        # Grammar Correction Layer (A2 - correct input before agents learn)
        try:
            clean_input = re.sub(r'\s+', ' ', text).strip()
            clean_input = clean_input.replace('ÔøΩ', '')
            clean_input = soft_grammar_correction(clean_input)
            text = clean_input
        except Exception as e:
            logger.warning(f"grammar preprocess failed: {e}")
        local_photo = None
        if update.message.photo:
            photo = update.message.photo[-1]
            file = await context.bot.get_file(photo.file_id)
            photo_bytes = await file.download_as_bytearray()
            filename = f"{int(time.time())}_{photo.file_unique_id}.jpg"
            local_path = os.path.join(PHOTO_CACHE_DIR, filename)
            with open(local_path, 'wb') as f:
                f.write(photo_bytes)
            local_photo = local_path
            if local_photo:
                user_photos.append(local_photo)
        raw_words = text.split()
        words = []
        clean_words = []
        for w in raw_words:
            clean_w = re.sub(r'[^\w]', '', w.lower())
            if clean_w and len(clean_w) <= 30:
                if word_significance.get(clean_w, 1.0) >= DYNAMIC_STOP_THRESHOLD:
                    words.append(w)
                    clean_words.append(clean_w)
        # ensure vector exists
        if 'vector' not in locals():
            vector = {}
        for clean_w in clean_words:
            markov_chain.setdefault(clean_w, [])

            # --- Enhanced priority weighting ---
            # emotional boost
            emo_strength = sum(vector.values())
            emo_boost = 1.0 + min(emo_strength * 0.15, 1.5)  # caps at +150%

            # resonance boost (if last known resonance exists)
            last_res = recent_messages[-1]['resonance'] if recent_messages else 0.0
            res_boost = 1.0 + last_res * 0.5  # up to +50%

            # rarity boost: rare words get higher priority
            rarity = 1.0 / (1.0 + word_significance.get(clean_w, 1.0))
            rarity_boost = 1.0 + min(rarity * 0.4, 0.6)  # up to +60%

            # combine boosts
            total_boost = emo_boost * res_boost * rarity_boost

            # update energy with boosted priority
            old_energy = word_weights.get(clean_w, 0.0)
            base_increment = np.random.uniform(0.8, 2.2)
            increment = base_increment * total_boost
            max_energy = 50.0
            new_energy = min(max_energy, old_energy + increment)
            word_weights[clean_w] = new_energy

            # update word significance (rarity-based)
            freq = word_weights.get(clean_w, 1)
            entropy_score = 1.0 / math.log(freq + 2)
            word_significance[clean_w] = (word_significance.get(clean_w, 0) * 0.85) + (entropy_score * 0.15)

            # --- Semantic classification & priority adjustment ---
            try:
                cls = determine_semantic_class(clean_w, text, vector)
            except Exception:
                cls = 'other'
            word_semantic[clean_w] = cls

            # base priority from significance (lower significance -> potentially higher priority if rare)
            base_priority = 1.0 / (1.0 + word_significance.get(clean_w, 1.0))

            # boost priority for emotionally-significant or social words
            emo_boost2 = 1.0 + min(sum(vector.values()) * 0.25, 1.0) if cls == 'emotion' else 1.0
            social_boost = 1.0 + 0.5 if cls == 'social' else 1.0
            action_boost = 1.0 + 0.25 if cls == 'action' else 1.0

            priority = base_priority * emo_boost2 * social_boost * action_boost
            # clamp and store
            priority = max(0.01, min(priority, 5.0))
            word_priority[clean_w] = priority

            # nudging word energy by semantic importance
            if cls in ('emotion', 'social'):
                # small immediate boost to energy for emotional/social words
                extra = 0.5 * (priority)
                word_weights[clean_w] = min(MAX_WORD_ENERGY, word_weights.get(clean_w, 0.0) + extra)

            if clean_w not in japanese_vocab:
                context.application.create_task(MultiLangLearner.learn_word(clean_w))
        for i in range(len(clean_words)-1):
            k, n = clean_words[i], clean_words[i+1]
            markov_chain.setdefault(k, []).append(n)
            if len(markov_chain[k]) > MAX_MARKOV_PER_WORD:
                markov_chain[k].pop(0)
            state = tuple(clean_words[max(0, i - CONTEXT_SIZE + 1):i + 1])
            context_chain.setdefault(state, []).append(clean_words[i + 1])
            if len(context_chain[state]) > MAX_MARKOV_PER_WORD:
                context_chain[state].pop(0)
        jp_words = re.findall(r'[\u3040-\u30ff\u4e00-\u9fff]+', text)
        for i in range(len(jp_words)-1):
            k, n = jp_words[i], jp_words[i+1]
            jp_markov_chain.setdefault(k, []).append(n)
            if len(jp_markov_chain[k]) > MAX_MARKOV_PER_WORD: jp_markov_chain[k].pop(0)
        vector = {k: sum(1 for kw in emotional_vectors[k] if kw in text) for k in emotional_vectors}
        energy = sum(word_weights.get(w, 0) for w in clean_words)
        msg_entry = {
            'text': " ".join(clean_words),
            'local_photo': local_photo,
            'energy': energy,
            'emotion_vector': vector,
            'emotion_strength': sum(vector.values()),
            'timestamp': time.time(),
            'timestamp_local': datetime.now(timezone(timedelta(hours=7))),
            'user': update.effective_user.username or update.effective_user.first_name,
            'resonance': 0.0
        }
        recent_messages.append(msg_entry)
        # --- Save to SQLite LTM ---
        try:
            save_message_to_db({**msg_entry, 'markov_chain': markov_chain, 'context_chain': context_chain})
        except Exception as e:
            logger.warning(f"LTM DB save_message_to_db error: {e}")
        # --- advanced_resonance_system: train and calculate resonance ---
        try:
            # Prepare features/target for training if enough history
            if len(recent_messages) > 10:
                def compute_message_value(m):
                    energy = m.get("energy", 0.0)
                    resonance = m.get("resonance", 0.0)
                    text = m.get("text", "") or ""
                    words = set(re.sub(r"[^\w]", " ", text.lower()).split())
                    rare_words = sum(1 for w in words if word_significance.get(w, 1.0) < 0.05)
                    uniqueness = rare_words / (len(words) + 1e-6)
                    return energy * 0.5 + resonance * 0.3 + uniqueness * 0.2

                train_candidates = list(recent_messages)[-min(len(recent_messages), 200):]
                values = [compute_message_value(m) for m in train_candidates]
                total_value = sum(values) or 1.0
                weights = [v / total_value for v in values]
                # Stochastic sampling by value
                train_samples = np.random.choice(train_candidates, size=min(len(train_candidates), 100), replace=True, p=np.array(weights)/np.sum(weights) if np.sum(weights) > 0 else None)
                train_features = []
                train_emos = []
                for m in train_samples:
                    text = m.get('text', '') or ''
                    detected = None
                    try:
                        detected = detect(text) if text else None
                    except:
                        detected = None
                    dominant_lang = yuma_identity.get('meta_analysis', {}).get('languages', {}).get('dominant')
                    if detected and dominant_lang and detected == dominant_lang:
                        lang_sync = 1.0
                    elif detected and dominant_lang and detected in yuma_identity.get('meta_analysis', {}).get('languages', {}).get('distribution', {}):
                        lang_sync = 0.5
                    else:
                        lang_sync = 0.0
                    last_vec = m.get('emotion_vector', {})
                    last_strength = sum(last_vec.values())
                    dominant_emotion = yuma_identity.get('meta_analysis', {}).get('dominant_emotions', {}).get('dominant')
                    if last_strength > 0:
                        emotion_sync = 1.0 if last_vec.get(dominant_emotion, 0) > 0 else 0.0
                    else:
                        emotion_sync = 1.0 if dominant_emotion else 0.0
                    top_words = set(yuma_identity.get('meta_analysis', {}).get('word_frequencies', {}).keys())
                    user_words = set(re.sub(r'[^\w]', ' ', text.lower()).split())
                    if not top_words:
                        semantic_sync = 0.0
                    else:
                        overlap = len(top_words & user_words)
                        semantic_sync = min(1.0, overlap / 3.0)
                    emo_vec = [
                        float(last_vec.get('joy', 0)),
                        float(last_vec.get('tension', 0)),
                        float(last_vec.get('flow', 0)),
                        float(last_vec.get('surprise', 0))
                    ]
                    energy = float(m.get('energy', 0.0))
                    word_count = float(len(user_words))
                    ts = m.get('timestamp', time.time())
                    hour = (datetime.fromtimestamp(ts).hour % 24) / 24.0
                    features = [
                        float(lang_sync), float(emotion_sync), float(semantic_sync),
                        emo_vec[0], emo_vec[1], emo_vec[2], emo_vec[3],
                        energy, word_count, hour,
                        0.0, 0.0
                    ]
                    train_features.append(features)
                    train_emos.append(emo_vec)
                    # Add to replay_buffer if emotionally saturated
                    if last_strength >= 2:
                        replay_buffer.add(features, emo_vec, [m.get('resonance', 0.0)])
                # --- Mini-batches and stochastic training ---
                batch_size = 16
                mini_epochs = 3
                device = next(advanced_resonance_system.parameters()).device
                if len(replay_buffer.buffer) >= batch_size:
                    X_all, EMOS_all, y_all = replay_buffer.sample(batch_size=len(replay_buffer.buffer))
                    if X_all is not None:
                        X_all = X_all.to(device)
                        EMOS_all = EMOS_all.to(device)
                        y_all = y_all.to(device)
                    # Shuffle
                    indices = np.arange(len(X_all))
                    np.random.shuffle(indices)
                    X_all = X_all[indices]
                    EMOS_all = EMOS_all[indices]
                    y_all = y_all[indices]
                    advanced_resonance_system.train()
                    for epoch in range(mini_epochs):
                        for i in range(0, len(X_all), batch_size):
                            X_batch = X_all[i:i+batch_size]
                            EMOS_batch = EMOS_all[i:i+batch_size]
                            y_batch = y_all[i:i+batch_size]
                            advanced_resonance_optimizer.zero_grad()
                            pred, uncertainty, attn_w, mem_out, emo_probs = advanced_resonance_system(X_batch, EMOS_batch)
                            loss = nn.functional.mse_loss(pred, y_batch)
                            loss.backward()
                            advanced_resonance_optimizer.step()
            # Calculate resonance for new message using advanced_resonance_system
            text = msg_entry.get('text', '') or ''
            detected = None
            try:
                detected = detect(text) if text else None
            except:
                detected = None
            dominant_lang = yuma_identity.get('meta_analysis', {}).get('languages', {}).get('dominant')
            if detected and dominant_lang and detected == dominant_lang:
                lang_sync = 1.0
            elif detected and dominant_lang and detected in yuma_identity.get('meta_analysis', {}).get('languages', {}).get('distribution', {}):
                lang_sync = 0.5
            else:
                lang_sync = 0.0
            last_vec = msg_entry.get('emotion_vector', {})
            last_strength = sum(last_vec.values())
            dominant_emotion = yuma_identity.get('meta_analysis', {}).get('dominant_emotions', {}).get('dominant')
            if last_strength > 0:
                emotion_sync = 1.0 if last_vec.get(dominant_emotion, 0) > 0 else 0.0
            else:
                emotion_sync = 1.0 if dominant_emotion else 0.0
            top_words = set(yuma_identity.get('meta_analysis', {}).get('word_frequencies', {}).keys())
            user_words = set(re.sub(r'[^\w]', ' ', text.lower()).split())
            if not top_words:
                semantic_sync = 0.0
            else:
                overlap = len(top_words & user_words)
                semantic_sync = min(1.0, overlap / 3.0)
            emo_vec = [
                float(last_vec.get('joy', 0)),
                float(last_vec.get('tension', 0)),
                float(last_vec.get('flow', 0)),
                float(last_vec.get('surprise', 0))
            ]
            energy = float(msg_entry.get('energy', 0.0))
            word_count = float(len(user_words))
            ts = msg_entry.get('timestamp', time.time())
            hour = (datetime.fromtimestamp(ts).hour % 24) / 24.0
            features = [
                float(lang_sync), float(emotion_sync), float(semantic_sync),
                emo_vec[0], emo_vec[1], emo_vec[2], emo_vec[3],
                energy, word_count, hour,
                0.0, 0.0
            ]
            device = next(advanced_resonance_system.parameters()).device
            x_tensor = torch.tensor([features], dtype=torch.float32, device=device)
            emo_tensor = torch.tensor([emo_vec], dtype=torch.float32, device=device)
            with torch.no_grad():
                r, uncertainty, attn_w, memory_enhanced, emotion_probs = advanced_resonance_system(x_tensor, emo_tensor)
                r_val = r.item() if hasattr(r, "item") else float(r)
            resonance_history.append({'ts': time.time(), 'resonance': r_val, 'user': msg_entry.get('user')})
            # Add experience to replay_buffer (stored on CPU)
            replay_buffer.add(features, emo_vec, [r_val])
            try:
                MAE.current_resonance = r_val
                msg_entry['resonance_state'] = RSM.get_state(r_val)
            except Exception:
                pass
        except Exception as e:
            logger.warning(f"resonance compute/train failed: {e}")
        total_energy = sum(word_weights.values())
        if total_energy >= RESO_THRESHOLD:
            await troll_text(update, context)
            for k in list(word_weights):
                word_weights[k] = max(0, word_weights[k] - RESO_THRESHOLD // 6)
                if word_weights[k] == 0: del word_weights[k]
        for w in list(word_significance.keys()):
            word_significance[w] *= 0.98
            if word_significance[w] < 0.001:
                word_significance.pop(w, None)
                markov_chain.pop(w, None)
                word_weights.pop(w, None)
        for w in list(word_weights.keys()):
            word_weights[w] *= 0.98
            if word_weights[w] < 0.01:
                word_weights.pop(w)
                markov_chain.pop(w, None)
                word_significance.pop(w, None)
                # --- –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø—Ä. ---
        save_data()
        update_yuma_identity()

        # === EmergentCore: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∂–∏–≤–æ–π ‚Üí –æ–Ω–∞ —Ä–∞–¥—É–µ—Ç—Å—è ===
        EmergentCore().on_user_activity()

        # === EmergentCore: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∂–∏–≤–æ–π ‚Üí –æ–Ω–∞ —Ä–∞–¥—É–µ—Ç—Å—è ===
        EmergentCore().on_user_activity()
    except Exception as e:
        logger.error(f"collect_words: {e}")

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import random
import time
from collections import Counter, deque
from typing import List, Dict, Any, Optional
import logging

# ----------------------------------------------------------------------
# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ, —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–æ–¥–µ)

class AgentGenome:
    def __init__(self, jp_ratio: float = 0.15, style_emoji: str = "sparkles", meme_affinity: float = 1.0):
        self.jp_ratio = float(jp_ratio)
        self.style_emoji = style_emoji
        self.meme_affinity = float(meme_affinity)

    def copy(self) -> 'AgentGenome':
        return AgentGenome(self.jp_ratio, self.style_emoji, self.meme_affinity)

    @staticmethod
    def random(style_choices: Optional[List[str]] = None) -> 'AgentGenome':
        if style_choices is None:
            style_choices = ["sparkles", "paw prints", "collision", "smirking cat", "robot", "game die", "water wave", "cyclone", "cherry blossom", "new moon"]
        return AgentGenome(
            jp_ratio=random.uniform(0.05, 0.35),
            style_emoji=random.choice(style_choices),
            meme_affinity=random.uniform(0.7, 1.3)
        )

    @staticmethod
    def crossover(g1: 'AgentGenome', g2: 'AgentGenome',
                  style_choices: Optional[List[str]] = None,
                  mutation_rate: float = 0.18) -> 'AgentGenome':
        if style_choices is None:
            style_choices = ["sparkles", "paw prints", "collision", "smirking cat", "robot", "game die", "water wave", "cyclone", "cherry blossom", "new moon"]

        # numeric blend
        jp = random.choice([g1.jp_ratio, g2.jp_ratio]) + random.uniform(-0.07, 0.07)
        jp = min(0.9, max(0.0, jp))

        meme_aff = (g1.meme_affinity + g2.meme_affinity) / 2.0 + random.uniform(-0.08, 0.08)
        meme_aff = min(2.0, max(0.3, meme_aff))

        style = random.choice([g1.style_emoji, g2.style_emoji])

        # mutation
        if random.random() < mutation_rate:
            style = random.choice([e for e in style_choices if e != style] or style_choices)
        if random.random() < mutation_rate:
            jp = random.uniform(0.05, 0.7)
        if random.random() < mutation_rate:
            meme_aff = random.uniform(0.5, 1.5)

        return AgentGenome(jp, style, meme_aff)


# --- Agent Interface and Variants ---
class AgentInterface:
    def __init__(self, name: str, genome: Optional[AgentGenome] = None):
        self.name = name
        self.energy = 0
        self.max_energy = 100
        self.status = "Idle"

        if genome is None:
            self.genome = AgentGenome.random()
        else:
            self.genome = genome.copy()

        # backward compatibility
        self.jp_ratio = self.genome.jp_ratio
        self.style_emoji = self.genome.style_emoji
        self.meme_affinity = self.genome.meme_affinity

    async def generate(self, phrase: str) -> str:
        return phrase

    def reward(self, value: int):
        self.energy = max(-50, min(self.energy + value, self.max_energy))

    async def speak(self, phrase: str, lang: str) -> str:
        jp_ratio = getattr(self, "jp_ratio", 0.0)
        if random.random() < jp_ratio:
            emoji = self.style_emoji if hasattr(self, 'style_emoji') else self.genome.style_emoji
            return phrase + f" {emoji}"
        return phrase



class AgentRandomFlow(AgentInterface):
    def __init__(self, name: str, genome: Optional[AgentGenome] = None):
        super().__init__(name, genome)

    async def generate(self, phrase: str) -> str:
        words = phrase.split()
        random.shuffle(words)
        return await self.speak(" ".join(words[:max(2, len(words)//2)]), "")


class AgentRelevantMeme(AgentInterface):
    def __init__(self, name: str, genome: Optional[AgentGenome] = None):
        super().__init__(name, genome)

    async def generate(self, phrase: str) -> str:
        return await self.speak(phrase, "")

# --- Allowlist –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ .pt ---
torch.serialization.add_safe_globals({
    'AgentRandomFlow': AgentRandomFlow,
    'AgentRelevantMeme': AgentRelevantMeme
})


# --- MultiAgentEngine with extended genome and Shader-like memory ---
class MultiAgentEngine:
    """
    Multi-agent Q-learning engine where each agent carries a small "shader-like"
    memory buffer. The shader memory is a lightweight numeric buffer that can
    be sampled with simple uniforms (resonance, time, energy) to produce a
    coherence score in [0,1]. This coherence is used alongside Q-values to
    bias agent selection and to guide evolution/crossover.
    """
    class ShaderMemory:
        """A tiny shader-like memory implemented as a vectorized kernel.
        It stores a small weight map and exposes `sample` and `mutate`.
        """
        def __init__(self, size=16, rng=None):
            self.size = int(size)
            self.rng = np.random.default_rng() if rng is None else rng
            # weights simulating a small shader kernel / texture
            self.weights = self.rng.normal(loc=0.0, scale=0.2, size=(self.size,)).astype(float)

        def sample(self, uniforms: dict) -> float:
            """Produce a coherence score 0..1 from uniforms.
            uniforms expected keys: resonance (0..1), energy, time (0..1)
            """
            r = float(uniforms.get('resonance', 0.0))
            e = float(uniforms.get('energy', 0.0))
            t = float(uniforms.get('time', 0.0))
            # combine uniforms into a small indexable pattern
            u = np.array([r, e % 1.0, t % 1.0], dtype=float)
            # simple mixing: dot with a hashed projection of weights
            proj = np.tanh(self.weights[:3] if self.size >= 3 else np.pad(self.weights, (0, 3 - len(self.weights))) )
            val = float(np.dot(proj, u))
            # map through sigmoid-like clamp to [0,1]
            coherence = 1.0 / (1.0 + np.exp(-5.0 * (val)))
            # small stochasticity for diversity
            coherence = float(np.clip(coherence + (self.rng.random() - 0.5) * 0.03, 0.0, 1.0))
            return coherence

        def mutate(self, strength: float = 0.12):
            """Apply small gaussian mutation to weights."""
            self.weights += self.rng.normal(scale=strength, size=self.weights.shape)
            # clamp to reasonable range
            self.weights = np.clip(self.weights, -2.0, 2.0)

        def crossover(self, other: 'MultiAgentEngine.ShaderMemory') -> 'MultiAgentEngine.ShaderMemory':
            """Create a child shader memory by mixing weights."""
            child = MultiAgentEngine.ShaderMemory(size=self.size, rng=self.rng)
            mask = self.rng.random(self.size) > 0.5
            child.weights = np.where(mask, self.weights, other.weights).copy()
            # slight smoothing
            child.weights = (child.weights + 0.02 * self.rng.normal(size=child.weights.shape))
            return child

    def __init__(self):
        style_choices = ["sparkles", "paw prints", "collision", "smirking cat", "robot", "game die", "water wave", "cyclone", "cherry blossom", "new moon"]
        self.agents: List[AgentInterface] = [
            AgentRandomFlow("RandomFlow", genome=AgentGenome.random(style_choices)),
            AgentRelevantMeme("RelevantMeme", genome=AgentGenome.random(style_choices))
        ]
        self.last_agent_index = 0
        self.max_agents = 5
        self.min_agents = 2

        for a in self.agents:
            a.jp_ratio = a.genome.jp_ratio
            a.style_emoji = a.genome.style_emoji
            a.meme_affinity = a.genome.meme_affinity
            # attach shader-like memory to each agent
            a.shader_memory = MultiAgentEngine.ShaderMemory(size=16)

        # Q-learning
        self.Q: Dict[tuple, List[float]] = {}
        self.epsilon = 0.15
        self.gamma = 0.85
        self.alpha = 0.33
        self.last_state: Optional[tuple] = None
        self.last_action: Optional[int] = None
        self.current_resonance = 0.0

    # ------------------------------------------------------------------
    # Q-learning helpers
    # ------------------------------------------------------------------
    def get_state(self) -> tuple:
        res = getattr(self, "current_resonance", 0.0)
        res_bin = int(res * 4.999)                     # 0..4
        last_action = self.last_action if self.last_action is not None else -1
        return (res_bin, last_action)

    def select_agent(self) -> AgentInterface:
        state = self.get_state()
        n_agents = len(self.agents)

        if state not in self.Q:
            self.Q[state] = [0.0 for _ in range(n_agents)]

        # Curiosity bonus: force exploration when resonance is too stable
        try:
            if getattr(self, "current_resonance", 0.0) > 0.9:
                self.epsilon = min(1.0, self.epsilon * 1.5)
        except Exception:
            pass

        # compute shader coherence for each agent and combine with Q as a soft bias
        uniforms = {
            'resonance': float(getattr(self, 'current_resonance', 0.0)),
            'energy': float(sum(getattr(a, 'energy', 0) for a in self.agents) / max(1, n_agents)),
            'time': (time.time() % 60) / 60.0
        }
        shader_scores = []
        for a in self.agents:
            try:
                sh = getattr(a, 'shader_memory', None)
                score = sh.sample(uniforms) if sh is not None else 0.5
            except Exception:
                score = 0.5
            shader_scores.append(score)

        # Normalize shader_scores to 0..1 and use as multiplicative preference to Q
        q_vals = self.Q[state]
        combined_scores = []
        for i in range(n_agents):
            q = float(q_vals[i])
            # map q to 0..1 via sigmoid-ish scaling (stabilize large values)
            q_norm = 1.0 / (1.0 + np.exp(-0.6 * (q)))
            combined = 0.6 * q_norm + 0.4 * float(shader_scores[i])
            combined_scores.append(combined)

        # epsilon-greedy but biased by combined_scores when exploiting
        if random.random() < self.epsilon:
            action = random.randint(0, n_agents - 1)
        else:
            max_val = max(combined_scores)
            best_actions = [i for i, v in enumerate(combined_scores) if v == max_val]
            action = random.choice(best_actions)

        self.last_state = state
        self.last_action = action
        self.last_agent_index = action
        return self.agents[action]

    # ------------------------------------------------------------------
    # Reward & evolution
    # ------------------------------------------------------------------
    def apply_reward(self, reward_signals: dict):
        agent = self.agents[self.last_agent_index]
        value = 0

        # positive
        if reward_signals.get("user_interaction"): value += 1
        if reward_signals.get("emotion"):          value += 2
        if reward_signals.get("media_success"):    value += 1

        # penalties
        if reward_signals.get("silence"):          value -= 1
        if reward_signals.get("logic_error"):      value -= 5
        if reward_signals.get("voice_error"):      value -= 3

        # resonance (with decay)
        res = reward_signals.get('resonance')
        if res is None and recent_messages:
            decayed = [
                m.get('resonance', 0.0) * (0.95 ** ((time.time() - m['timestamp']) / 60))
                for m in list(recent_messages)[-20:]
            ]
            res = sum(decayed) / len(decayed) if decayed else 0.0

        if res is not None:
            if res >= RESONANCE_THRESHOLD:
                value += 3
            else:
                value -= 1

        # diversity bonus
        style_counts = Counter(getattr(a, "style_emoji", "‚Äî") for a in self.agents)
        agent_style = getattr(agent, "style_emoji", "‚Äî")
        rarity_bonus = 1.0 / (1 + style_counts.get(agent_style, 0))
        value = int(value * (1 + rarity_bonus))

        # small shader-guided reward: if agent shader coherence was high, add micro-bonus
        try:
            sh = getattr(agent, 'shader_memory', None)
            if sh is not None:
                coherence = sh.sample({'resonance': self.current_resonance, 'energy': value, 'time': (time.time()%60)/60.0})
                if coherence > 0.7:
                    value += 1
        except Exception:
            pass

        # Q-update
        if self.last_state is not None and self.last_action is not None:
            state = self.last_state
            action = self.last_action
            n_agents = len(self.agents)

            if state not in self.Q or len(self.Q[state]) != n_agents:
                self.Q[state] = [0.0 for _ in range(n_agents)]

            next_state = self.get_state()
            if next_state not in self.Q or len(self.Q[next_state]) != n_agents:
                self.Q[next_state] = [0.0 for _ in range(n_agents)]

            max_next_q = max(self.Q[next_state])
            old_q = self.Q[state][action]
            new_q = old_q + self.alpha * (value + self.gamma * max_next_q - old_q)
            self.Q[state][action] = new_q

        agent.reward(value)

        # persist
        resonance_history.append({'ts': time.time(), 'resonance': res, 'agent': agent.name})
        self.evolve_if_needed()

    # ------------------------------------------------------------------
    # Evolution
    # ------------------------------------------------------------------
    def evolve_if_needed(self):
        candidates = [a for a in self.agents if getattr(a, "energy", 0) >= 80]
        style_choices = ["sparkles", "paw prints", "collision", "smirking cat", "robot", "game die", "water wave", "cyclone", "cherry blossom", "new moon"]

        # reproduction
        while candidates and len(self.agents) < self.max_agents:
            parent = candidates.pop(0)
            others = [a for a in self.agents if a is not parent]
            parent2 = random.choice(others) if others else parent
            child = self.crossover_mutate(parent, parent2, style_choices)
            self.agents.append(child)
            logger.info(f"MultiAgentEngine: –ê–≥–µ–Ω—Ç '{parent.name}' –ø–æ—Ä–æ–¥–∏–ª –º—É—Ç–∞–Ω—Ç–∞ '{child.name}'")

        # elimination
        while len(self.agents) > self.min_agents:
            weakest = min(self.agents, key=lambda a: getattr(a, "energy", 0))
            if weakest.energy <= -20:
                logger.info(f"MultiAgentEngine: –ê–≥–µ–Ω—Ç '{weakest.name}' —É–¥–∞–ª—ë–Ω –∏–∑-–∑–∞ –Ω–∏–∑–∫–æ–π —ç–Ω–µ—Ä–≥–∏–∏ ({weakest.energy})")
                self.agents.remove(weakest)
            else:
                break

    def crossover_mutate(self, parent1: AgentInterface, parent2: AgentInterface, style_choices: List[str]) -> AgentInterface:
        genome = AgentGenome.crossover(parent1.genome, parent2.genome, style_choices=style_choices)
        cls = parent1.__class__
        new_name = parent1.name + "_mut" + str(random.randint(100, 999))
        child = cls(new_name, genome)
        child.jp_ratio = genome.jp_ratio
        child.style_emoji = genome.style_emoji
        child.meme_affinity = genome.meme_affinity
        # combine shader memories
        try:
            sh1 = getattr(parent1, 'shader_memory', None)
            sh2 = getattr(parent2, 'shader_memory', None)
            if sh1 is not None and sh2 is not None:
                child.shader_memory = sh1.crossover(sh2)
                # small mutation
                child.shader_memory.mutate(strength=0.08)
            else:
                child.shader_memory = MultiAgentEngine.ShaderMemory(size=16)
        except Exception:
            child.shader_memory = MultiAgentEngine.ShaderMemory(size=16)
        child.energy = 0
        return child

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
class DavidRLSystem:
    def __init__(self):
        self.q_table = {}
        self.learning_rate = 0.1
        self.discount_factor = 0.95
        self.epsilon = 0.2
        self.state = None
        self.action = None

    def get_state(self, obs):
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –≤ —Ö–µ—à–∏—Ä—É–µ–º–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        return tuple(obs)

    def select_action(self, state, actions):
        # –≠–ø—Å–∏–ª–æ–Ω-–∂–∞–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
        if random.random() < self.epsilon:
            action = random.choice(actions)
        else:
            q_vals = [self.q_table.get((state, a), 0.0) for a in actions]
            max_q = max(q_vals)
            best_actions = [a for a, q in zip(actions, q_vals) if q == max_q]
            action = random.choice(best_actions)
        self.state = state
        self.action = action
        return action

    def update(self, next_state, reward, actions):
        prev_q = self.q_table.get((self.state, self.action), 0.0)
        next_qs = [self.q_table.get((next_state, a), 0.0) for a in actions]
        max_next_q = max(next_qs) if next_qs else 0.0
        new_q = prev_q + self.learning_rate * (reward + self.discount_factor * max_next_q - prev_q)
        self.q_table[(self.state, self.action)] = new_q

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
DAVID_RL = DavidRLSystem()


# ----------------------------------------------------------------------
# Visualization
# ----------------------------------------------------------------------
def visualize_agents(agents: List[AgentInterface]):
    import matplotlib.cm as cm
    from matplotlib.lines import Line2D

    x = [getattr(a, "jp_ratio", getattr(a, "genome", AgentGenome()).jp_ratio) for a in agents]
    y = [getattr(a, "energy", 0) for a in agents]
    style_list = [getattr(a, "style_emoji", getattr(a, "genome", AgentGenome()).style_emoji) for a in agents]
    size = [80 + 120 * getattr(a, "meme_affinity", getattr(a, "genome", AgentGenome()).meme_affinity) for a in agents]

    unique_styles = list(sorted(set(style_list)))
    color_map = {s: cm.rainbow(i / max(1, len(unique_styles)-1)) for i, s in enumerate(unique_styles)}
    colors = [color_map[s] for s in style_list]

    fig, ax = plt.subplots(figsize=(7, 5))
    scatter = ax.scatter(x, y, s=size, c=colors, alpha=0.7, edgecolor='k')
    for i, a in enumerate(agents):
        ax.annotate(a.name, (x[i], y[i]), fontsize=8, ha='left', va='bottom')

    ax.set_xlabel("jp_ratio")
    ax.set_ylabel("–≠–Ω–µ—Ä–≥–∏—è")
    ax.set_title("–ü–æ–ø—É–ª—è—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ (jp_ratio vs —ç–Ω–µ—Ä–≥–∏—è, —Ü–≤–µ—Ç=emoji, —Ä–∞–∑–º–µ—Ä=affinity)")

    legend_elements = [
        Line2D([0], [0], marker='o', color='w', label=s,
               markerfacecolor=color_map[s], markersize=10)
        for s in unique_styles
    ]
    ax.legend(handles=legend_elements, title="style_emoji")
    plt.tight_layout()
    plt.show()


# ----------------------------------------------------------------------
# Init
# ----------------------------------------------------------------------
MAE = MultiAgentEngine()
MAE.current_resonance = 0.0


# ----------------------------------------------------------------------
# Resonance state machine
# ----------------------------------------------------------------------
class ResonanceStateMachine:
    def get_state(self, resonance: float) -> str:
        if resonance >= 0.75: return "high"
        if resonance >= 0.35: return "medium"
        return "low"

    def attraction_multiplier(self, resonance: float) -> float:
        return 1.0 + (resonance * 1.5)


RSM = ResonanceStateMachine()


# ----------------------------------------------------------------------
# Language detection buffer
# ----------------------------------------------------------------------
_lang_history = deque(maxlen=10)

def detect_context_lang(new_text: str) -> str:
    global _lang_history
    try:
        from langdetect import detect
        lang = detect(new_text)
    except Exception:
        lang = "en"

    if len(new_text.strip()) < 3 and _lang_history:
        return _lang_history[-1]

    _lang_history.append(lang)
    if len(_lang_history) > 3:
        lang = max(set(_lang_history), key=list(_lang_history).count)
    return lang
#
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# –¢–†–û–õ–õ–¨: –†–£–°–°–ö–ò–ô –¢–ï–ö–°–¢ ‚Üí –Ø–ü–û–ù–°–ö–ò–ô –ì–û–õ–û–° (—Å –∑–∞—â–∏—Ç–æ–π)
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# --- Import pydub AudioSegment, effects, low_pass_filter, high_pass_filter with fallback ---
from pydub import AudioSegment, effects
try:
    from pydub.effects import low_pass_filter, high_pass_filter
except ImportError:
    # fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π pydub
    def low_pass_filter(audio, cutoff):
        return audio
    def high_pass_filter(audio, cutoff):
        return audio

# --- VoiceIntegrityEngine ---
class VoiceIntegrityEngine:
    def __init__(self):
        self.min_length_ms = 1200
        self.max_length_ms = 12000

    def verify_segment(self, segment):
        if segment is None or len(segment) < 200:
            return False
        if segment.rms < 50:
            return False
        return True

    def rebuild_if_needed(self, segments):
        repaired = []
        for seg in segments:
            if not self.verify_segment(seg):
                seg = Sine(440).to_audio_segment(duration=400).apply_gain(-12)
            repaired.append(seg)
        return repaired

    def finalize(self, segments):
        from pydub.effects import normalize, low_pass_filter, high_pass_filter
        result = AudioSegment.silent(duration=0)
        for seg in segments:
            result += seg.fade_in(20).fade_out(20)
        result = high_pass_filter(result, 80)
        result = low_pass_filter(result, 7500)
        result = normalize(result)
        if len(result) < self.min_length_ms:
            result += Sine(440).to_audio_segment(duration=self.min_length_ms - len(result)).apply_gain(-18)
        return result

def make_anime_voice(text: str, voice_lang: str = None) -> AudioSegment:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–ª–∞–≤–Ω—ã–π anime-–≥–æ–ª–æ—Å –¥–ª—è –ª—é–±–æ–≥–æ —è–∑—ã–∫–∞ –æ–¥–Ω–∏–º —Å–µ–≥–º–µ–Ω—Ç–æ–º
    —Å —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏: pitch, speed, volume, glitch, sigh.
    """
    if not text.strip():
        text = "„Å´„ÇÉ„Çì"

    if not voice_lang:
        voice_lang = detect_context_lang(text)

    integrity = VoiceIntegrityEngine()
    # --- Mastering FX chain class ---
    class MasterFXChain:
        def __init__(self):
            self.ott_intensity = 0.6
            self.reverb_mix = 0.25
            self.grain_density = 0.15

        def apply(self, audio: AudioSegment) -> AudioSegment:
            processed = effects.normalize(audio)
            processed = processed.compress_dynamic_range(threshold=-20.0, ratio=3.5)
            echo = processed.fade_out(300).apply_gain(-12)
            mixed = processed.overlay(echo, gain_during_overlay=-6)
            if random.random() < self.grain_density:
                chunks = [mixed[i:i+150] for i in range(0, len(mixed), 150)]
                random.shuffle(chunks)
                mixed = sum(chunks)
            return mixed.fade_in(40).fade_out(40)
    master_fx = MasterFXChain()

    anime_sighs = ["„Åµ„ÅÖ", "„Å´„ÇÉ", "„Åà„Å∏", "„ÅÜ„Éº„Çì", "„Å´„ÇÉ„Çì", "„ÅØ„ÅÅ", "„ÅÜ„ÅÖ", "„Åç„ÇÉ", "„Åª„Çè", "„Åç„ÇÖ„Çì", "„Çè„ÅÅ"]

    # TTS –Ω–∞ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ä–∞–∑—É
    try:
        buf = io.BytesIO()
        tts = gTTS(text=text, lang=voice_lang)
        tts.write_to_fp(buf)
        buf.seek(0)
        result_audio = AudioSegment.from_file(buf, format="mp3")
    except Exception as e:
        logger.warning(f"make_anime_voice: gTTS fail for full text '{text}': {e}")
        result_audio = Sine(440).to_audio_segment(duration=800)

    # Pitch shift –∏ speed modulation
    # base_pitch = random.uniform(-2, 2)
    # --- –ñ–µ–Ω—Å–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω pitch ---
    base_pitch = random.uniform(3, 6)
    orig_rate = result_audio.frame_rate
    new_rate = int(orig_rate * (2.0 ** (base_pitch / 12.0)))
    result_audio = result_audio._spawn(result_audio.raw_data, overrides={'frame_rate': new_rate})
    result_audio = result_audio.set_frame_rate(24000)
    result_audio = low_pass_filter(high_pass_filter(result_audio, 180), 7000)
    base_speed = random.uniform(1.05, 1.15)
    try:
        result_audio = result_audio.speedup(playback_speed=base_speed, chunk_size=120, crossfade=18)
    except:
        pass
    base_volume = random.uniform(-1.5, 1.5)
    result_audio += base_volume

    # –°–ª—É—á–∞–π–Ω—ã–µ sighs –≤ –∫–æ–Ω—Ü–µ
    if random.random() < 0.3:
        sigh = random.choice(anime_sighs)
        try:
            buf2 = io.BytesIO()
            tts2 = gTTS(text=sigh, lang=voice_lang)
            tts2.write_to_fp(buf2)
            buf2.seek(0)
            sigh_audio = AudioSegment.from_file(buf2, format="mp3")
            sigh_audio += random.uniform(-1.5, 1.5)
            result_audio += sigh_audio
        except:
            pass

    # --- –ü—Ä–∏–º–µ–Ω–∏—Ç—å –º–∞—Å—Ç–µ—Ä-—Ü–µ–ø–æ—á–∫—É ---
    result_audio = master_fx.apply(result_audio)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ integrity
    result_audio = integrity.finalize([result_audio])

    # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å –≤ –ø–∞–º—è—Ç—å ---
    if 'voice_memory' not in globals():
        globals()['voice_memory'] = {}
    try:
        key = f"{int(time.time())}_{voice_lang}"
        buf_mem = io.BytesIO()
        result_audio.export(buf_mem, format="wav")
        buf_mem.seek(0)
        voice_memory[key] = buf_mem.getvalue()
    except Exception as e:
        logger.warning(f"make_anime_voice: failed to store voice_memory: {e}")

    return result_audio

async def troll_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global word_significance
    """
    –¢—Ä–æ–ª–ª—å-—Ñ—É–Ω–∫—Ü–∏—è: —Å–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç, —á—Ç–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å ‚Äî —Ç–µ–∫—Å—Ç, –≥–æ–ª–æ—Å, –º–µ–º, –∏–ª–∏ –∏—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏—é.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç troll_phrase (—Ö–∞–æ—Ç–∏—á–Ω—ã–π —Ç–µ–∫—Å—Ç), make_anime_voice (–≥–æ–ª–æ—Å), generate_meme (–º–µ–º).
    """
    try:
        # Determine detected language from incoming message (unified)
        detected_lang = "unknown"
        try:
            user_text = None
            if hasattr(update, "message") and update.message:
                if update.message.text:
                    user_text = update.message.text
                elif update.message.caption:
                    user_text = update.message.caption
            if user_text:
                detected_lang = detect(user_text)
        except Exception:
            detected_lang = "unknown"

        # --- 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–∞–æ—Ç–∏—á–Ω—ã–π troll_phrase (–ª–æ–≥–∏–∫–∞ –ø—Ä–µ–∂–Ω—è—è) ---
        # --- LTM: Load recent messages from SQLite for pattern use ---
        try:
            ltm_messages = load_recent_messages(limit=20)
        except Exception as e:
            logger.warning(f"LTM DB load_recent_messages error: {e}")
            ltm_messages = []
        all_words = []
        clean_words = []
        # Use LTM messages if available, else fallback to recent_messages
        source_msgs = ltm_messages if ltm_messages else recent_messages
        for m in source_msgs:
            if m.get('text'):
                ws = [w for w in m['text'].split() if not is_stop_word(w) and len(w) <= 30]
                all_words.extend(ws)
                clean_words.extend(ws)
        if reddit_meme_texts:
            extra = [
                re.sub(r'[^\w]', '', w.lower())
                for w in random.choice(reddit_meme_texts).split()[:6]
                if re.sub(r'[^\w]', '', w.lower()) and not is_stop_word(re.sub(r'[^\w]', '', w.lower())) and len(re.sub(r'[^\w]', '', w.lower())) <= 30
            ]
            all_words.extend(extra)
            clean_words.extend(extra)
        def markov_generate(chain, start=None, length=8):
            # Prefer context-based generation for more meaningful sequences
            words = []
            keys = list(chain.keys())
            if not keys:
                return words

            word = start or random.choice(keys)
            words.append(word)

            for _ in range(length - 1):
                # Try context_chain first for coherence
                ctx_state = tuple(words[-2:]) if len(words) >= 2 else None
                next_word = None

                if ctx_state and ctx_state in context_chain and context_chain[ctx_state]:
                    next_word = random.choice(context_chain[ctx_state])
                else:
                    # fallback to regular markov
                    candidates = chain.get(word)
                    if candidates:
                        next_word = random.choice(candidates)

                if not next_word:
                    next_word = random.choice(keys)

                words.append(next_word)
                word = next_word

            return words
        rus_words = [w for w in all_words if not re.search(r'[\u3040-\u30ff\u4e00-\u9fff]', w)]
        rus_words = [w for w in rus_words if not is_stop_word(w)]
        if rus_words and markov_chain:
            start_word = random.choice(rus_words)
            markov_phrase = markov_generate(markov_chain, start=start_word, length=random.randint(5, 9))
        else:
            markov_phrase = rus_words[:random.randint(5, 9)]
        mixed = []
        for w in markov_phrase:
            jp = japanese_vocab.get(w)
            if jp and random.random() < 0.5:
                mixed.append(jp)
                if random.random() < 0.3:
                    mixed.append(w)
            else:
                mixed.append(w)
        if random.random() < 0.18 and japanese_vocab:
            mixed.append(random.choice(list(japanese_vocab.values())))
        if rus_words:
            if random.random() < 0.7:
                mixed.insert(0, random.choice(rus_words))
            if random.random() < 0.7:
                mixed.append(random.choice(rus_words))
        base_phrase = " ".join(mixed).strip()
        agent = MAE.select_agent()
        troll_phrase = await agent.speak(base_phrase, detected_lang)

        # --- –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ---
        # detected_lang already set above (unified)

        # --- –ï—Å–ª–∏ —è–∑—ã–∫ —Ä—É—Å—Å–∫–∏–π, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5% —è–ø–æ–Ω—Å–∫–∏—Ö —Å–ª–æ–≤ –≤ troll_phrase –¥–ª—è —Ç–µ–∫—Å—Ç–∞ ---
        troll_phrase_for_text = troll_phrase
        # Full Japanese mode if chat is in Japanese
        if detected_lang == "ja":
            troll_phrase_for_text = troll_phrase  # allow full JP
        elif detected_lang in ["ru", "en", "fr"]:
            words = troll_phrase.split()
            total_words = len(words)
            # –ú–∞–∫—Å–∏–º—É–º 5% —è–ø–æ–Ω—Å–∫–∏—Ö —Å–ª–æ–≤, –º–∏–Ω–∏–º—É–º 1 –µ—Å–ª–∏ —Ñ—Ä–∞–∑–∞ –∫–æ—Ä–æ—Ç–∫–∞—è
            max_jp = max(1, int(total_words * 0.05 + 0.5))
            # –ò–Ω–¥–µ–∫—Å—ã —è–ø–æ–Ω—Å–∫–∏—Ö —Å–ª–æ–≤
            jp_indices = [i for i, w in enumerate(words) if re.search(r'[\u3040-\u30ff\u4e00-\u9fff]', w)]
            if len(jp_indices) > max_jp:
                # –û—Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ max_jp —è–ø–æ–Ω—Å–∫–∏—Ö —Å–ª–æ–≤, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–∞–º–µ–Ω–∏–º –Ω–∞ —Ä—É—Å—Å–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏ –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–ª–∏ —É–±–µ—Ä—ë–º
                jp_keep = set(jp_indices[:max_jp])
                new_words = []
                for i, w in enumerate(words):
                    if re.search(r'[\u3040-\u30ff\u4e00-\u9fff]', w):
                        if i in jp_keep:
                            new_words.append(w)
                        else:
                            rus = jp_rus_map.get(w)
                            if rus:
                                new_words.append(rus)
                            # –∏–Ω–∞—á–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —è–ø–æ–Ω—Å–∫–æ–µ —Å–ª–æ–≤–æ
                    else:
                        new_words.append(w)
                troll_phrase_for_text = " ".join(new_words)
            else:
                troll_phrase_for_text = troll_phrase
        else:
            # If not Russian/English/French ‚Äî keep default but reduce JP to 30%
            words = troll_phrase.split()
            new_words = []
            for w in words:
                if re.search(r'[\u3040-\u30ff\u4e00-\u9fff]', w) and random.random() > 0.3:
                    # replace extra JP word if exists russian equivalent
                    rus = jp_rus_map.get(w)
                    if rus:
                        new_words.append(rus)
                else:
                    new_words.append(w)
            troll_phrase_for_text = " ".join(new_words)

        # --- –ò—Å–ø–æ–ª—å–∑—É–µ–º —è–∑—ã–∫ —á–∞—Ç–∞ –¥–ª—è TTS, —è–ø–æ–Ω—Å–∫–∏–π —Ç–æ–ª—å–∫–æ –∫–∞–∫ –≤—Å—Ç–∞–≤–∫–∞ ---
        # final_audio_text ‚Äî —Ç–µ–∫—Å—Ç –Ω–∞ —è–∑—ã–∫–µ —á–∞—Ç–∞
        words = troll_phrase_for_text.split()
        # –≤—Å—Ç–∞–≤–ª—è–µ–º 5‚Äì10% —è–ø–æ–Ω—Å–∫–∏—Ö —Å–ª–æ–≤ —Å–ª—É—á–∞–π–Ω–æ
        num_jp_insert = max(1, int(len(words) * 0.07))
        jp_candidates = list(japanese_vocab.values())
        for _ in range(num_jp_insert):
            idx = random.randint(0, len(words)-1)
            words[idx] = random.choice(jp_candidates)
        final_audio_text = " ".join(words)

        # --- 3. –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ—Ç–ø—Ä–∞–≤–∫–∏ ---
        # 0: —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç, 1: —Ç–æ–ª—å–∫–æ –≥–æ–ª–æ—Å, 2: —Ç–æ–ª—å–∫–æ –º–µ–º, 3: —Ç–µ–∫—Å—Ç+–º–µ–º, 4: –≥–æ–ª–æ—Å+–º–µ–º, 5: —Ç–µ–∫—Å—Ç+–≥–æ–ª–æ—Å, 6: –≤—Å—ë
        mode = random.choices(
            population=[0, 1, 2, 3, 4, 5, 6],
            weights=[0.18, 0.2, 0.13, 0.18, 0.13, 0.09, 0.09],  # —Å—É–º–º–∞—Ä–Ω–æ 1.0
            k=1
        )[0]

        sent_something = False
        errors = []

        # --- –¢–µ–∫—Å—Ç ---
        async def send_text():
            if not hasattr(update, "message") or not update.message:
                logger.warning("send_text: update.message –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return False
            try:
                # –û–±—Ä–µ–∑–∞–µ–º troll_phrase –¥–æ –º–∞–∫—Å–∏–º—É–º –¥–≤—É—Ö —Å–ª–æ–≤
                short_troll_phrase = " ".join(troll_phrase_for_text.split()[:2])
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é –æ—Ç–ø—Ä–∞–≤–∫—É —Å retry
                success = await safe_reply_text(update.message, short_troll_phrase)
                if not success:
                    logger.error("send_text: –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ safe_reply_text –Ω–µ —É–¥–∞–ª–∏—Å—å")
                return success
            except Exception as e:
                logger.error(f"troll_text: send_text unexpected error: {e}")
                return False

        # --- –ì–æ–ª–æ—Å ---
        async def send_voice():
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –≥–æ–ª–æ—Å–∞
                try:
                    detected = detect(final_audio_text)
                except Exception:
                    detected = None
                voice_lang = detected if detected in ["ja", "ru", "en", "fr"] else detected_lang

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è TTS
                audio_text_words = final_audio_text.split()
                if voice_lang in ["ru", "en", "fr"]:
                    filtered_words = []
                    for w in audio_text_words:
                        if re.search(r'[\u3040-\u30ff\u4e00-\u9fff]', w):
                            rus = jp_rus_map.get(w)
                            if rus:
                                filtered_words.append(rus)
                        else:
                            filtered_words.append(w)
                    voice_text = " ".join(filtered_words).strip()
                else:
                    voice_text = final_audio_text.strip()
                if len(voice_text) < 3:
                    voice_text = "„Å´„ÇÉ„Çì"

         # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ MutRes –¥–ª—è pitch/volume
                try:
                    mutres = MutRes()
                    current_state = mutres.state.copy()
                    res_energy = float(np.mean(np.abs(current_state)))
                    pitch_shift = max(-2.0, min(2.0, (res_energy - 0.5) * 4.0))
                    volume_mod = 1.0 + (res_energy - 0.5) * 0.6
                except Exception as e:
                    logger.warning(f"MutRes integration in voice failed: {e}")
                    pitch_shift = 0.0
                    volume_mod = 1.0

                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∏–º–µ-–≥–æ–ª–æ—Å–∞ —á–µ—Ä–µ–∑ make_anime_voice
                def tts_generate(text, lang, pitch_shift=0.0, volume_mod=1.0):
                    audio = make_anime_voice(text, voice_lang=lang)
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º pitch/volume –º–æ–¥—É–ª—è—Ü–∏—é
                    orig_rate = audio.frame_rate
                    new_rate = int(orig_rate * (2.0 ** (pitch_shift / 12.0)))
                    audio = audio._spawn(audio.raw_data, overrides={'frame_rate': new_rate})
                    audio = audio.set_frame_rate(22050)
                    audio += (volume_mod - 1.0) * 5.0
                    return audio

                audio_segment = await asyncio.to_thread(tts_generate, voice_text, voice_lang, pitch_shift, volume_mod)

                # –≠–∫—Å–ø–æ—Ä—Ç –≤ ogg –¥–ª—è Telegram
                buf_out = io.BytesIO()
                audio_segment.export(buf_out, format="ogg", codec="libopus", bitrate="48k")
                buf_out.seek(0)
                duration = int(len(audio_segment) / 1000)

                await update.message.reply_voice(
                    voice=InputFile(buf_out, f"yuma_voice_{voice_lang}.ogg"),
                    duration=duration
                )
                return True
            except Exception as e:
                logger.error(f"troll_text send_voice error: {e}")
                await update.message.reply_text("")
                return False

        # --- –ú–µ–º ---
        async def send_meme():
            try:
                await generate_meme(update, context)
                return True
            except Exception as e:
                logger.error(f"troll_text: send_meme error: {e}")
                errors.append("meme")
                return False

        # --- –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏—è –ø–æ mode ---
        if mode == 0:
            await send_text()
        elif mode == 1:
            await send_voice()
        elif mode == 2:
            await send_meme()
        elif mode == 3:
            await send_text()
            await send_meme()
        elif mode == 4:
            await send_voice()
            await send_meme()
        elif mode == 5:
            await send_text()
            await send_voice()
        elif mode == 6:
            await send_text()
            await send_voice()
            await send_meme()
        # Apply basic reward: user interaction from troll
        reward = {"user_interaction": True}
        MAE.apply_reward(reward)
    except Exception as e:
        logger.error(f"troll_text: {e}")
        await update.message.reply_text("")

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# –ö–æ–º–∞–Ω–¥—ã
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "„Å´„ÇÉ„Å£„ÅØ„ÉºÔºÅ<b>Yuma Nami v3.2</b> Ëµ∑ÂãïÔºÅ\n"
        "–ß–∞—Ç + Reddit ‚Üí —è–ø–æ–Ω—Å–∫–∏–π –≥–æ–ª–æ—Å. –ù–∏–∫–∞–∫–∏—Ö —à–∞–±–ª–æ–Ω–æ–≤.\n\n"
        "<b>–ö–æ–º–∞–Ω–¥—ã:</b>\n"
        "/troll ‚Äî —Ç—Ä–æ–ª–ª—å\n"
        "/status ‚Äî —Å—Ç–∞—Ç—É—Å\n"
        "/reset_memory ‚Äî —Å–±—Ä–æ—Å\n"
        "/set_threshold &lt;—á–∏—Å–ª–æ&gt; ‚Äî –ø–æ—Ä–æ–≥\n"
        "/fetch_reddit ‚Äî Reddit —Å–µ–π—á–∞—Å",
        parse_mode='HTML'
    )


async def status(update: Update, context: ContextTypes.DEFAULT_TYPE):
    total_energy = sum(word_weights.values())
    meta = yuma_identity["meta_analysis"]
    msg = (
        f"<b>Yuna Status</b>\n\n"
        f"–°–æ–æ–±—â–µ–Ω–∏–π: <code>{len(recent_messages)}</code>\n"
        f"–°–ª–æ–≤: <code>{len(word_weights)}</code>\n"
        f"–≠–Ω–µ—Ä–≥–∏—è: <code>{total_energy}/{RESO_THRESHOLD}</code>\n"
        f"–Ø–ø–æ–Ω—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å: <code>{len(japanese_vocab)}</code>\n"
        f"Reddit: <code>{len(reddit_meme_texts)}</code> —Ç–µ–∫—Å—Ç–æ–≤\n"
        f"–≠–º–æ—Ü–∏—è: <code>{meta['dominant_emotions'].get('dominant', '‚Äî')}</code>"
    )
    await safe_reply_text(update.message, msg, parse_mode='HTML')

# --- Evolution command ---
async def evolution(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        msg = "<b>Evolution Status</b>\n\n"
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤ –ø–æ —ç–Ω–µ—Ä–≥–∏–∏, –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        sorted_agents = sorted(MAE.agents, key=lambda x: getattr(x, "energy", 0), reverse=True)

        for a in sorted_agents:
            emoji = getattr(a, "style_emoji", "‚Äî")
            ratio = getattr(a, "jp_ratio", None)
            if isinstance(ratio, float):
                ratio_display = f"{ratio:.2f}"
            else:
                ratio_display = "‚Äî"
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä —ç–Ω–µ—Ä–≥–∏–∏ (0-100)
            energy = getattr(a, "energy", 0)
            max_energy = getattr(a, "max_energy", 100)
            filled = int((energy / max_energy) * 10)  # 10 —Å–∏–º–≤–æ–ª–æ–≤
            bar = "‚ñà" * filled + "‚ñë" * (10 - filled)

            # –°—Ç–∞—Ç—É—Å –∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
            status = getattr(a, "status", "Idle")
            
            msg += (
                f"<code>{a.name}</code> | "
                f"{bar} <b>{energy}/{max_energy}</b> | "
                f"Emoji: {emoji} | "
                f"JP%: {ratio_display} | "
                f"Status: {status}\n"
            )
        
        await update.message.reply_text(msg, parse_mode='HTML')
    except Exception as e:
        logger.error(f"evolution cmd: {e}")
        await update.message.reply_text("Evolution error‚Ä¶")

async def reset_memory(update: Update, context: ContextTypes.DEFAULT_TYPE):
    init_data()
    await update.message.reply_text("–ü–∞–º—è—Ç—å —Å—Ç—ë—Ä—Ç–∞. –•–∞–æ—Å –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω...")

async def set_threshold(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not context.args or not context.args[0].isdigit():
        await update.message.reply_text("<b>/set_threshold 30</b>", parse_mode='HTML')
        return
    global RESO_THRESHOLD
    RESO_THRESHOLD = int(context.args[0])
    save_data()
    await update.message.reply_text(f"–ü–æ—Ä–æ–≥: <b>{RESO_THRESHOLD}</b>", parse_mode='HTML')

async def fetch_reddit(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("Reddit –ø–∞—Ä—Å–∏–Ω–≥... [loading]")
    memes = await fetch_reddit_json()
    if not memes:
        memes = await fetch_reddit_fallback()
    integrate_reddit_memes(memes)
    await update.message.reply_text(f"–ì–æ—Ç–æ–≤–æ! +{len(memes)} –º–µ–º–æ–≤. –•–∞–æ—Å —É—Å–∏–ª–µ–Ω! ‚ú®")

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
# ==============================================================================
# Resonance Synchronization Protocol (Experimental Multi-Node Network Section)
# ==============================================================================
# This section implements the basis for a resonance synchronization protocol
# across multiple Yuma nodes using WebSockets. Each node broadcasts and receives
# resonance packets containing its resonance state, and merges incoming packets
# into its local resonance field.
#
# Packet fields: node_id, timestamp, energy_vector, entropy_level, dominant_emotion
#
# Requirements: websockets (pip install websockets), asyncio

# mutres_core_async.py
import os
import uuid
import time
import json
import logging
import asyncio
import numpy as np
import websockets

class MutRes:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω–æ–µ —è–¥—Ä–æ, –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–µ –∏ –ª—ë–≥–∫–æ–µ –ø–æ —Ä–µ—Å—É—Ä—Å–∞–º.
    - –û—Ç–ª–æ–∂–µ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç: –Ω–µ —Å–æ–∑–¥–∞—ë—Ç —Ç–∞—Å–∫ –≤ __init__, –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç async start().
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio primitives –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –º—è–≥–∫–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.
    - –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö: get_state() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ø–∏—é –º–∞—Å—Å–∏–≤–∞ –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, state_size=10, decay=0.95, update_interval=0.12):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑
        if getattr(self, "_initialized", False):
            return
        self._initialized = True

        self.state_size = int(state_size)
        self.decay = float(decay)
        self.update_interval = float(update_interval)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fast PRNG
        self._rng = np.random.default_rng()
        self._state = np.zeros(self.state_size, dtype=float)

        # –ó–∞—â–∏—Ç–∞ –¥–æ—Å—Ç—É–ø–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–∞—Å–∫–æ–º
        self._lock = asyncio.Lock()
        self._task = None
        self._stop_event = asyncio.Event()

        # callbacks may be sync or async - keep small
        self.callbacks = []

        # lightweight logger
        self._log = logging.getLogger("MutRes")
        self._log.setLevel(logging.WARNING)

    async def start(self):
        """Start background update loop. Safe to call multiple times."""
        if self._task is not None and not self._task.done():
            return
        self._stop_event.clear()
        loop = asyncio.get_running_loop()
        self._task = loop.create_task(self._loop(), name="MutRes._loop")

    async def stop(self):
        """Signal background loop to stop and wait for it to finish."""
        self._stop_event.set()
        if self._task is not None:
            try:
                await asyncio.wait_for(self._task, timeout=2.0)
            except asyncio.TimeoutError:
                try:
                    self._task.cancel()
                except Exception:
                    pass
        self._task = None

    async def _loop(self):
        """Background coroutine that updates state asynchronously and calls callbacks.
        It keeps work minimal and yields to the event loop frequently.
        """
        try:
            while not self._stop_event.is_set():
                await self._update_state_once()
                # sleep is the main yield point; keep interval modest
                try:
                    await asyncio.wait_for(self._stop_event.wait(), timeout=self.update_interval)
                except asyncio.TimeoutError:
                    pass
        except asyncio.CancelledError:
            pass
        except Exception as e:
            # Never raise from background loop ‚Äî just log
            self._log.warning(f"MutRes loop exception: {e}")

    async def _update_state_once(self):
        """One non-blocking update of internal state. Keeps lock time very short."""
        # Create tiny noise vector using numpy rng (fast)
        noise = (self._rng.random(self.state_size) - 0.5) * 0.02
        # Do arithmetic outside lock, then swap in under lock
        new_state = self._state * self.decay + noise
        # Short critical section
        async with self._lock:
            self._state = new_state
            state_copy = self._state.copy()
        # Call callbacks but do not await them here ‚Äî schedule them to run later
        for cb in list(self.callbacks):
            try:
                res = cb(state_copy)
                if asyncio.iscoroutine(res):
                    # schedule coroutine but don't await
                    asyncio.create_task(res)
            except Exception as e:
                self._log.warning(f"MutRes callback error: {e}")

    def attach(self, func):
        """Attach a callback (sync or async). Callbacks will be scheduled after updates."""
        if callable(func):
            self.callbacks.append(func)

    def detach(self, func):
        try:
            self.callbacks.remove(func)
        except ValueError:
            pass

    def get_state(self):
        """Return a thread-safe copy of current state (sync). Very cheap."""
        # No await required; do a fast local copy under lock if loop is running
        if self._lock.locked():
            # If lock is locked, try a non-blocking approach
            return self._state.copy()
        # Safe copy
        return self._state.copy()

    @property
    def state(self):
        return self.get_state()

    # Backwards-compatible stop method name
    def stop_sync(self):
        # schedule stop in background
        try:
            asyncio.create_task(self.stop())
        except Exception:
            pass
AUTOSAVE_INTERVAL = 60  # —Å–µ–∫—É–Ω–¥

async def autosave_loop():
    await asyncio.sleep(5)  # —Å—Ç–∞—Ä—Ç–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
    while True:
        try:
            await save_ltm_pt()
            logger.info("–ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ .pt –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è .pt: {e}")
        await asyncio.sleep(AUTOSAVE_INTERVAL)

# –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ—Å–µ–π–≤–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ


# --- –ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---
mutres = MutRes()
# schedule the MutRes background loop safely in the running event loop
try:
    asyncio.create_task(mutres.start())
except Exception:
    # If we're not in an event loop at import time, the main() will start it explicitly
    pass

# Unique node ID (persist or random per session)
NODE_ID = os.environ.get("YUMA_NODE_ID") or str(uuid.uuid4())

# Local resonance field (can be more sophisticated)
local_resonance_field = {
    "energy_vector": [0.0, 0.0, 0.0, 0.0],  # e.g., [joy, tension, flow, surprise]
    "entropy_level": 0.0,
    "dominant_emotion": "flow"
}

def build_resonance_packet():
    """
    Build a resonance packet for broadcasting.
    Returns a dict ready for JSON serialization.
    """
    packet = {
        "node_id": NODE_ID,
        "timestamp": time.time(),
        "energy_vector": list(local_resonance_field.get("energy_vector", [0.0, 0.0, 0.0, 0.0])),
        "entropy_level": float(local_resonance_field.get("entropy_level", 0.0)),
        "dominant_emotion": str(local_resonance_field.get("dominant_emotion", "flow"))
    }
    return packet

def update_resonance_field(packet):
    """
    Merge incoming resonance packet into local resonance.
    This is a naive merge: weighted average for energy_vector/entropy, update dominant_emotion by majority.
    """
    if not isinstance(packet, dict) or packet.get("node_id") == NODE_ID:
        return
    try:
        # Weighted average with incoming
        lv = local_resonance_field.get("energy_vector", [0.0, 0.0, 0.0, 0.0])
        pv = packet.get("energy_vector", [0.0, 0.0, 0.0, 0.0])
        local_resonance_field["energy_vector"] = [
            (a + b) / 2.0 for a, b in zip(lv, pv)
        ]
        le = float(local_resonance_field.get("entropy_level", 0.0))
        pe = float(packet.get("entropy_level", 0.0))
        local_resonance_field["entropy_level"] = (le + pe) / 2.0
        # Dominant emotion: majority voting (for demo, just use incoming)
        local_resonance_field["dominant_emotion"] = packet.get("dominant_emotion", local_resonance_field.get("dominant_emotion"))
    except Exception as e:
        logger.warning(f"Resonance sync merge error: {e}")

async def resonance_sync_loop(
    uri="ws://localhost:8765",
    interval=5.0
):
    """
    Periodically broadcasts and receives resonance packets over WebSockets.
    This is a basic loop: connects to a WebSocket server, sends local state, receives others'.
    """
    while True:
        try:
            async with websockets.connect(uri) as ws:
                logger.info(f"[ResonanceSync] Connected to {uri}")
                while True:
                    # Send local resonance packet
                    packet = build_resonance_packet()
                    await ws.send(json.dumps(packet))
                    # Try to receive one or more packets
                    try:
                        resp = await asyncio.wait_for(ws.recv(), timeout=interval)
                        if resp:
                            try:
                                incoming = json.loads(resp)
                                update_resonance_field(incoming)
                                logger.debug(f"[ResonanceSync] Merged packet from {incoming.get('node_id')}")
                            except Exception as e:
                                logger.warning(f"Resonance sync JSON error: {e}")
                    except asyncio.TimeoutError:
                        pass  # No packet received this interval
                    await asyncio.sleep(interval)
        except Exception as e:
            logger.warning(f"[ResonanceSync] Connection error: {e}. Retrying in 10s.")
            await asyncio.sleep(10)

# To start the resonance sync loop, call:
# asyncio.create_task(resonance_sync_loop("ws://your_server:8765"))
# This is only a demo; in production, use a real WebSocket server and robust error handling.
# –ó–∞–ø—É—Å–∫
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì
async def main():
    app = Application.builder().token("paste your token").build()
    await app.initialize()
    WEBAPP_URL = "https://0penagi.github.io/YunaNami/"
# –≤ handler start:
    from telegram import WebAppInfo, InlineKeyboardMarkup, InlineKeyboardButton
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("troll", troll_text))
    app.add_handler(CommandHandler("status", status))
    app.add_handler(CommandHandler("evolution", evolution))
    app.add_handler(CommandHandler("reset_memory", reset_memory))
    app.add_handler(CommandHandler("set_threshold", set_threshold))
    app.add_handler(CommandHandler("fetch_reddit", fetch_reddit))
    app.add_handler(MessageHandler(filters.PHOTO | (filters.TEXT & ~filters.COMMAND), collect_words))
    app.add_handler(MessageHandler(filters.VOICE, handle_voice))
    # --- –ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---
    mutres = MutRes()
    try:
        await mutres.start()
    except Exception as e:
        logger.warning(f"MutRes start failed: {e}")
    asyncio.create_task(auto_reddit_fetch())
    asyncio.create_task(auto_rss_fetch())
    asyncio.create_task(autosave_loop())
    logger.info("Yuma Nami v3.2 ‚Äî –ü–û–õ–ù–´–ô –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ë–û–¢ –ó–ê–ü–£–©–ï–ù")
    try:
        await app.run_polling()
    except RuntimeError as e:
        if "Cannot close a running event loop" in str(e):
            pass
        else:
            raise

if __name__ == '__main__':
    import nest_asyncio
    nest_asyncio.apply()
    load_data()
    load_ltm_pt()  # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞–º—è—Ç—å –∏–∑ .pt –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ, –µ—Å–ª–∏ –µ—Å—Ç—å
    import asyncio
    asyncio.run(main())

# --- –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ .pt –∫–∞–∂–¥—ã–µ N —Å–µ–∫—É–Ω–¥ ---
